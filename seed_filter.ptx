//
// Generated by NVIDIA NVVM Compiler
//
// Compiler Build ID: CL-35059454
// Cuda compilation tools, release 12.6, V12.6.85
// Based on NVVM 7.0.1
//

.version 8.5
.target sm_70
.address_size 64

	// .globl	_Z17find_seeds_kernelmj12FilterParamsPmPVi

.visible .entry _Z17find_seeds_kernelmj12FilterParamsPmPVi(
	.param .u64 _Z17find_seeds_kernelmj12FilterParamsPmPVi_param_0,
	.param .u32 _Z17find_seeds_kernelmj12FilterParamsPmPVi_param_1,
	.param .align 4 .b8 _Z17find_seeds_kernelmj12FilterParamsPmPVi_param_2[20],
	.param .u64 _Z17find_seeds_kernelmj12FilterParamsPmPVi_param_3,
	.param .u64 _Z17find_seeds_kernelmj12FilterParamsPmPVi_param_4
)
{
	.local .align 16 .b8 	__local_depot0[32];
	.reg .b64 	%SP;
	.reg .b64 	%SPL;
	.reg .pred 	%p<529>;
	.reg .b16 	%rs<32>;
	.reg .b32 	%r<1253>;
	.reg .f64 	%fd<567>;
	.reg .b64 	%rd<1182>;


	mov.u64 	%SPL, __local_depot0;
	ld.param.u64 	%rd209, [_Z17find_seeds_kernelmj12FilterParamsPmPVi_param_0];
	ld.param.u32 	%r328, [_Z17find_seeds_kernelmj12FilterParamsPmPVi_param_1];
	ld.param.v4.u8 	{%rs5, %rs6, %rs7, %rs8}, [_Z17find_seeds_kernelmj12FilterParamsPmPVi_param_2+16];
	ld.param.u64 	%rd211, [_Z17find_seeds_kernelmj12FilterParamsPmPVi_param_4];
	ld.param.u32 	%r325, [_Z17find_seeds_kernelmj12FilterParamsPmPVi_param_2+4];
	ld.param.u32 	%r324, [_Z17find_seeds_kernelmj12FilterParamsPmPVi_param_2];
	cvta.to.global.u64 	%rd1, %rd211;
	add.u64 	%rd2, %SPL, 0;
	mov.u32 	%r329, %ntid.x;
	mov.u32 	%r330, %ctaid.x;
	mov.u32 	%r331, %tid.x;
	mad.lo.s32 	%r1, %r330, %r329, %r331;
	setp.ge.u32 	%p1, %r1, %r328;
	@%p1 bra 	$L__BB0_269;

	ld.volatile.global.u32 	%r332, [%rd1];
	setp.ne.s32 	%p2, %r332, 0;
	@%p2 bra 	$L__BB0_269;
	bra.uni 	$L__BB0_2;

$L__BB0_269:
	ret;

$L__BB0_2:
	cvt.s64.s32 	%rd213, %r1;
	add.s64 	%rd3, %rd213, %rd209;
	and.b32  	%r333, %r325, %r324;
	setp.eq.s32 	%p3, %r333, -1;
	@%p3 bra 	$L__BB0_92;

	shr.u64 	%rd214, %rd3, 56;
	cvt.u32.u64 	%r336, %rd3;
	shr.u64 	%rd215, %rd3, 48;
	cvt.u32.u64 	%r337, %rd215;
	shr.u64 	%rd216, %rd3, 40;
	shr.u64 	%rd217, %rd3, 32;
	cvt.u32.u64 	%r338, %rd217;
	shr.u64 	%rd218, %rd3, 24;
	shr.u32 	%r339, %r336, 16;
	mov.u32 	%r1134, 16;
	and.b32  	%r340, %r336, 255;
	cvt.u16.u64 	%rs9, %rd3;
	shr.u16 	%rs10, %rs9, 8;
	cvt.u32.u16 	%r341, %rs10;
	prmt.b32 	%r342, %r341, %r340, 30212;
	cvt.u32.u64 	%r343, %rd218;
	and.b32  	%r344, %r339, 255;
	prmt.b32 	%r345, %r343, %r344, 30212;
	prmt.b32 	%r4, %r345, %r342, 4180;
	cvt.u32.u64 	%r346, %rd216;
	and.b32  	%r347, %r338, 255;
	prmt.b32 	%r348, %r346, %r347, 30212;
	cvt.u32.u64 	%r349, %rd214;
	and.b32  	%r350, %r337, 255;
	prmt.b32 	%r351, %r349, %r350, 30212;
	prmt.b32 	%r5, %r351, %r348, 4180;
	mov.u32 	%r352, 1060126512;
	mov.u32 	%r353, 1026899007;
	st.local.v4.u32 	[%rd2], {%r353, %r352, %r4, %r5};
	mov.f64 	%fd517, 0d3FF0000000000000;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r354}, %fd517;
	}
	and.b32  	%r6, %r354, 2147483647;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r355, %temp}, %fd517;
	}
	mov.b64 	%fd528, {%r355, %r6};
	mul.f64 	%fd2, %fd528, 0d4350000000000000;
	mov.u32 	%r1133, 15;

$L__BB0_4:
	mov.u32 	%r8, %r1134;
	mov.u32 	%r1134, %r1133;
	cvt.s64.s32 	%rd219, %r1134;
	add.s64 	%rd220, %rd2, %rd219;
	ld.local.s8 	%rs11, [%rd220];
	cvt.rn.f64.s16 	%fd148, %rs11;
	mov.f64 	%fd149, 0d3FF1FB9C7406BB70;
	div.rn.f64 	%fd150, %fd149, %fd517;
	mul.f64 	%fd151, %fd150, %fd148;
	mul.f64 	%fd152, %fd151, 0d400921FB54442EEA;
	cvt.rn.f64.s32 	%fd153, %r8;
	fma.rn.f64 	%fd4, %fd153, 0d400921FB54442EEA, %fd152;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r9}, %fd4;
	}
	and.b32  	%r356, %r9, 2147483647;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r357, %temp}, %fd4;
	}
	mov.b64 	%fd514, {%r357, %r356};
	setp.gt.u32 	%p4, %r356, 2146435071;
	setp.gt.u32 	%p5, %r6, 2146435071;
	or.pred  	%p6, %p4, %p5;
	@%p6 bra 	$L__BB0_21;
	bra.uni 	$L__BB0_5;

$L__BB0_21:
	setp.le.f64 	%p41, %fd528, 0d7FF0000000000000;
	setp.le.f64 	%p42, %fd514, 0d7FF0000000000000;
	and.pred  	%p43, %p42, %p41;
	@%p43 bra 	$L__BB0_23;
	bra.uni 	$L__BB0_22;

$L__BB0_23:
	setp.eq.f64 	%p44, %fd514, 0d7FF0000000000000;
	selp.f64 	%fd517, 0dFFF8000000000000, %fd4, %p44;
	bra.uni 	$L__BB0_24;

$L__BB0_5:
	setp.eq.f64 	%p7, %fd528, 0d0000000000000000;
	mov.f64 	%fd517, 0dFFF8000000000000;
	@%p7 bra 	$L__BB0_24;

	setp.ltu.f64 	%p8, %fd514, %fd528;
	mov.f64 	%fd517, %fd4;
	@%p8 bra 	$L__BB0_24;

	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r358}, %fd514;
	}
	shr.u32 	%r1135, %r358, 20;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r359}, %fd528;
	}
	shr.u32 	%r1136, %r359, 20;
	setp.ne.s32 	%p9, %r1135, 0;
	@%p9 bra 	$L__BB0_9;

	mul.f64 	%fd514, %fd514, 0d4350000000000000;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r360}, %fd514;
	}
	shr.u32 	%r361, %r360, 20;
	add.s32 	%r1135, %r361, -54;

$L__BB0_9:
	setp.ne.s32 	%p10, %r1136, 0;
	mov.f64 	%fd515, %fd528;
	@%p10 bra 	$L__BB0_11;

	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r362}, %fd2;
	}
	shr.u32 	%r363, %r362, 20;
	add.s32 	%r1136, %r363, -54;
	mov.f64 	%fd515, %fd2;

$L__BB0_11:
	mov.b64 	%rd222, %fd514;
	and.b64  	%rd223, %rd222, 4503599627370495;
	or.b64  	%rd1090, %rd223, 4503599627370496;
	mov.b64 	%rd224, %fd515;
	and.b64  	%rd225, %rd224, 4503599627370495;
	or.b64  	%rd6, %rd225, 4503599627370496;
	sub.s32 	%r1142, %r1135, %r1136;
	not.b32 	%r364, %r1135;
	add.s32 	%r365, %r1136, %r364;
	max.s32 	%r366, %r365, -1;
	add.s32 	%r17, %r366, %r1135;
	mov.u32 	%r367, 2;
	sub.s32 	%r368, %r367, %r1136;
	add.s32 	%r369, %r368, %r17;
	and.b32  	%r1138, %r369, 3;
	setp.eq.s32 	%p11, %r1138, 0;
	@%p11 bra 	$L__BB0_13;

$L__BB0_12:
	.pragma "nounroll";
	sub.s64 	%rd226, %rd1090, %rd6;
	mov.b64 	%fd155, %rd226;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r370}, %fd155;
	}
	setp.lt.s32 	%p12, %r370, 0;
	selp.b64 	%rd1093, %rd1090, %rd226, %p12;
	shl.b64 	%rd1090, %rd1093, 1;
	add.s32 	%r1142, %r1142, -1;
	add.s32 	%r1138, %r1138, -1;
	setp.ne.s32 	%p13, %r1138, 0;
	@%p13 bra 	$L__BB0_12;

$L__BB0_13:
	mov.u32 	%r371, 1;
	sub.s32 	%r372, %r371, %r1136;
	add.s32 	%r373, %r372, %r17;
	setp.lt.u32 	%p14, %r373, 3;
	@%p14 bra 	$L__BB0_18;

	not.b32 	%r374, %r1142;
	max.s32 	%r375, %r374, -4;
	add.s32 	%r376, %r1142, %r375;
	add.s32 	%r24, %r376, 4;
	shr.u32 	%r377, %r24, 2;
	add.s32 	%r378, %r377, 1;
	and.b32  	%r1141, %r378, 3;
	setp.eq.s32 	%p15, %r1141, 0;
	@%p15 bra 	$L__BB0_16;

$L__BB0_15:
	.pragma "nounroll";
	sub.s64 	%rd228, %rd1090, %rd6;
	mov.b64 	%fd156, %rd228;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r379}, %fd156;
	}
	setp.lt.s32 	%p16, %r379, 0;
	selp.b64 	%rd229, %rd1090, %rd228, %p16;
	shl.b64 	%rd230, %rd229, 1;
	sub.s64 	%rd231, %rd230, %rd6;
	mov.b64 	%fd157, %rd231;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r380}, %fd157;
	}
	setp.lt.s32 	%p17, %r380, 0;
	selp.b64 	%rd232, %rd230, %rd231, %p17;
	shl.b64 	%rd233, %rd232, 1;
	sub.s64 	%rd234, %rd233, %rd6;
	mov.b64 	%fd158, %rd234;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r381}, %fd158;
	}
	setp.lt.s32 	%p18, %r381, 0;
	selp.b64 	%rd235, %rd233, %rd234, %p18;
	shl.b64 	%rd236, %rd235, 1;
	sub.s64 	%rd237, %rd236, %rd6;
	mov.b64 	%fd159, %rd237;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r382}, %fd159;
	}
	setp.lt.s32 	%p19, %r382, 0;
	selp.b64 	%rd1093, %rd236, %rd237, %p19;
	shl.b64 	%rd1090, %rd1093, 1;
	add.s32 	%r1142, %r1142, -4;
	add.s32 	%r1141, %r1141, -1;
	setp.ne.s32 	%p20, %r1141, 0;
	@%p20 bra 	$L__BB0_15;

$L__BB0_16:
	setp.lt.u32 	%p21, %r24, 12;
	@%p21 bra 	$L__BB0_18;

$L__BB0_17:
	sub.s64 	%rd238, %rd1090, %rd6;
	mov.b64 	%fd160, %rd238;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r383}, %fd160;
	}
	setp.lt.s32 	%p22, %r383, 0;
	selp.b64 	%rd239, %rd1090, %rd238, %p22;
	shl.b64 	%rd240, %rd239, 1;
	sub.s64 	%rd241, %rd240, %rd6;
	mov.b64 	%fd161, %rd241;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r384}, %fd161;
	}
	setp.lt.s32 	%p23, %r384, 0;
	selp.b64 	%rd242, %rd240, %rd241, %p23;
	shl.b64 	%rd243, %rd242, 1;
	sub.s64 	%rd244, %rd243, %rd6;
	mov.b64 	%fd162, %rd244;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r385}, %fd162;
	}
	setp.lt.s32 	%p24, %r385, 0;
	selp.b64 	%rd245, %rd243, %rd244, %p24;
	shl.b64 	%rd246, %rd245, 1;
	sub.s64 	%rd247, %rd246, %rd6;
	mov.b64 	%fd163, %rd247;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r386}, %fd163;
	}
	setp.lt.s32 	%p25, %r386, 0;
	selp.b64 	%rd248, %rd246, %rd247, %p25;
	shl.b64 	%rd249, %rd248, 1;
	sub.s64 	%rd250, %rd249, %rd6;
	mov.b64 	%fd164, %rd250;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r387}, %fd164;
	}
	setp.lt.s32 	%p26, %r387, 0;
	selp.b64 	%rd251, %rd249, %rd250, %p26;
	shl.b64 	%rd252, %rd251, 1;
	sub.s64 	%rd253, %rd252, %rd6;
	mov.b64 	%fd165, %rd253;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r388}, %fd165;
	}
	setp.lt.s32 	%p27, %r388, 0;
	selp.b64 	%rd254, %rd252, %rd253, %p27;
	shl.b64 	%rd255, %rd254, 1;
	sub.s64 	%rd256, %rd255, %rd6;
	mov.b64 	%fd166, %rd256;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r389}, %fd166;
	}
	setp.lt.s32 	%p28, %r389, 0;
	selp.b64 	%rd257, %rd255, %rd256, %p28;
	shl.b64 	%rd258, %rd257, 1;
	sub.s64 	%rd259, %rd258, %rd6;
	mov.b64 	%fd167, %rd259;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r390}, %fd167;
	}
	setp.lt.s32 	%p29, %r390, 0;
	selp.b64 	%rd260, %rd258, %rd259, %p29;
	shl.b64 	%rd261, %rd260, 1;
	sub.s64 	%rd262, %rd261, %rd6;
	mov.b64 	%fd168, %rd262;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r391}, %fd168;
	}
	setp.lt.s32 	%p30, %r391, 0;
	selp.b64 	%rd263, %rd261, %rd262, %p30;
	shl.b64 	%rd264, %rd263, 1;
	sub.s64 	%rd265, %rd264, %rd6;
	mov.b64 	%fd169, %rd265;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r392}, %fd169;
	}
	setp.lt.s32 	%p31, %r392, 0;
	selp.b64 	%rd266, %rd264, %rd265, %p31;
	shl.b64 	%rd267, %rd266, 1;
	sub.s64 	%rd268, %rd267, %rd6;
	mov.b64 	%fd170, %rd268;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r393}, %fd170;
	}
	setp.lt.s32 	%p32, %r393, 0;
	selp.b64 	%rd269, %rd267, %rd268, %p32;
	shl.b64 	%rd270, %rd269, 1;
	sub.s64 	%rd271, %rd270, %rd6;
	mov.b64 	%fd171, %rd271;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r394}, %fd171;
	}
	setp.lt.s32 	%p33, %r394, 0;
	selp.b64 	%rd272, %rd270, %rd271, %p33;
	shl.b64 	%rd273, %rd272, 1;
	sub.s64 	%rd274, %rd273, %rd6;
	mov.b64 	%fd172, %rd274;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r395}, %fd172;
	}
	setp.lt.s32 	%p34, %r395, 0;
	selp.b64 	%rd275, %rd273, %rd274, %p34;
	shl.b64 	%rd276, %rd275, 1;
	sub.s64 	%rd277, %rd276, %rd6;
	mov.b64 	%fd173, %rd277;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r396}, %fd173;
	}
	setp.lt.s32 	%p35, %r396, 0;
	selp.b64 	%rd278, %rd276, %rd277, %p35;
	shl.b64 	%rd279, %rd278, 1;
	sub.s64 	%rd280, %rd279, %rd6;
	mov.b64 	%fd174, %rd280;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r397}, %fd174;
	}
	setp.lt.s32 	%p36, %r397, 0;
	selp.b64 	%rd281, %rd279, %rd280, %p36;
	shl.b64 	%rd282, %rd281, 1;
	sub.s64 	%rd283, %rd282, %rd6;
	mov.b64 	%fd175, %rd283;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r398}, %fd175;
	}
	setp.lt.s32 	%p37, %r398, 0;
	selp.b64 	%rd1093, %rd282, %rd283, %p37;
	shl.b64 	%rd1090, %rd1093, 1;
	add.s32 	%r32, %r1142, -16;
	setp.gt.s32 	%p38, %r1142, 15;
	mov.u32 	%r1142, %r32;
	@%p38 bra 	$L__BB0_17;

$L__BB0_18:
	and.b64  	%rd21, %rd1093, 9223372036854775807;
	setp.eq.s64 	%p39, %rd21, 0;
	mov.f64 	%fd516, 0d0000000000000000;
	@%p39 bra 	$L__BB0_20;

	mov.b64 	%fd177, %rd21;
	mul.f64 	%fd178, %fd177, 0d4350000000000000;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r399}, %fd178;
	}
	shr.u32 	%r400, %r399, 20;
	mov.u32 	%r401, 55;
	sub.s32 	%r402, %r401, %r400;
	sub.s32 	%r403, %r1136, %r402;
	shl.b64 	%rd284, %rd21, %r402;
	setp.lt.s32 	%p40, %r403, 1;
	mov.u32 	%r404, 1;
	sub.s32 	%r405, %r404, %r403;
	shr.u64 	%rd285, %rd284, %r405;
	add.s32 	%r406, %r403, -1;
	cvt.u64.u32 	%rd286, %r406;
	shl.b64 	%rd287, %rd286, 52;
	add.s64 	%rd288, %rd287, %rd284;
	selp.b64 	%rd289, %rd285, %rd288, %p40;
	mov.b64 	%fd516, %rd289;

$L__BB0_20:
	and.b32  	%r407, %r9, -2147483648;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r408}, %fd516;
	}
	or.b32  	%r409, %r408, %r407;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r410, %temp}, %fd516;
	}
	mov.b64 	%fd517, {%r410, %r409};
	bra.uni 	$L__BB0_24;

$L__BB0_22:
	mov.f64 	%fd179, 0d3FF0000000000000;
	add.rn.f64 	%fd517, %fd4, %fd179;

$L__BB0_24:
	add.s32 	%r1133, %r1134, -1;
	setp.gt.s32 	%p45, %r1134, 0;
	@%p45 bra 	$L__BB0_4;

	setp.gt.u32 	%p526, %r6, 2146435071;
	fma.rn.f64 	%fd15, %fd517, 0d3FFB96C889463A4C, 0d4001135C51C7F716;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r34}, %fd15;
	}
	and.b32  	%r411, %r34, 2147483647;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r412, %temp}, %fd15;
	}
	mov.b64 	%fd518, {%r412, %r411};
	setp.gt.u32 	%p47, %r411, 2146435071;
	or.pred  	%p48, %p47, %p526;
	@%p48 bra 	$L__BB0_42;
	bra.uni 	$L__BB0_26;

$L__BB0_42:
	setp.le.f64 	%p83, %fd528, 0d7FF0000000000000;
	setp.le.f64 	%p84, %fd518, 0d7FF0000000000000;
	and.pred  	%p85, %p84, %p83;
	@%p85 bra 	$L__BB0_44;
	bra.uni 	$L__BB0_43;

$L__BB0_44:
	setp.eq.f64 	%p86, %fd518, 0d7FF0000000000000;
	selp.f64 	%fd521, 0dFFF8000000000000, %fd15, %p86;
	bra.uni 	$L__BB0_45;

$L__BB0_26:
	setp.eq.f64 	%p49, %fd528, 0d0000000000000000;
	mov.f64 	%fd521, 0dFFF8000000000000;
	@%p49 bra 	$L__BB0_45;

	setp.ltu.f64 	%p50, %fd518, %fd528;
	mov.f64 	%fd521, %fd15;
	@%p50 bra 	$L__BB0_45;

	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r413}, %fd518;
	}
	shr.u32 	%r1144, %r413, 20;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r414}, %fd528;
	}
	shr.u32 	%r1145, %r414, 20;
	setp.ne.s32 	%p51, %r1144, 0;
	@%p51 bra 	$L__BB0_30;

	mul.f64 	%fd518, %fd518, 0d4350000000000000;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r415}, %fd518;
	}
	shr.u32 	%r416, %r415, 20;
	add.s32 	%r1144, %r416, -54;

$L__BB0_30:
	setp.ne.s32 	%p52, %r1145, 0;
	mov.f64 	%fd519, %fd528;
	@%p52 bra 	$L__BB0_32;

	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r417}, %fd2;
	}
	shr.u32 	%r418, %r417, 20;
	add.s32 	%r1145, %r418, -54;
	mov.f64 	%fd519, %fd2;

$L__BB0_32:
	mov.b64 	%rd291, %fd518;
	and.b64  	%rd292, %rd291, 4503599627370495;
	or.b64  	%rd1098, %rd292, 4503599627370496;
	mov.b64 	%rd293, %fd519;
	and.b64  	%rd294, %rd293, 4503599627370495;
	or.b64  	%rd23, %rd294, 4503599627370496;
	sub.s32 	%r1151, %r1144, %r1145;
	not.b32 	%r419, %r1144;
	add.s32 	%r420, %r1145, %r419;
	max.s32 	%r421, %r420, -1;
	add.s32 	%r42, %r421, %r1144;
	mov.u32 	%r422, 2;
	sub.s32 	%r423, %r422, %r1145;
	add.s32 	%r424, %r423, %r42;
	and.b32  	%r1147, %r424, 3;
	setp.eq.s32 	%p53, %r1147, 0;
	@%p53 bra 	$L__BB0_34;

$L__BB0_33:
	.pragma "nounroll";
	sub.s64 	%rd295, %rd1098, %rd23;
	mov.b64 	%fd181, %rd295;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r425}, %fd181;
	}
	setp.lt.s32 	%p54, %r425, 0;
	selp.b64 	%rd1101, %rd1098, %rd295, %p54;
	shl.b64 	%rd1098, %rd1101, 1;
	add.s32 	%r1151, %r1151, -1;
	add.s32 	%r1147, %r1147, -1;
	setp.ne.s32 	%p55, %r1147, 0;
	@%p55 bra 	$L__BB0_33;

$L__BB0_34:
	mov.u32 	%r426, 1;
	sub.s32 	%r427, %r426, %r1145;
	add.s32 	%r428, %r427, %r42;
	setp.lt.u32 	%p56, %r428, 3;
	@%p56 bra 	$L__BB0_39;

	not.b32 	%r429, %r1151;
	max.s32 	%r430, %r429, -4;
	add.s32 	%r431, %r1151, %r430;
	add.s32 	%r49, %r431, 4;
	shr.u32 	%r432, %r49, 2;
	add.s32 	%r433, %r432, 1;
	and.b32  	%r1150, %r433, 3;
	setp.eq.s32 	%p57, %r1150, 0;
	@%p57 bra 	$L__BB0_37;

$L__BB0_36:
	.pragma "nounroll";
	sub.s64 	%rd297, %rd1098, %rd23;
	mov.b64 	%fd182, %rd297;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r434}, %fd182;
	}
	setp.lt.s32 	%p58, %r434, 0;
	selp.b64 	%rd298, %rd1098, %rd297, %p58;
	shl.b64 	%rd299, %rd298, 1;
	sub.s64 	%rd300, %rd299, %rd23;
	mov.b64 	%fd183, %rd300;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r435}, %fd183;
	}
	setp.lt.s32 	%p59, %r435, 0;
	selp.b64 	%rd301, %rd299, %rd300, %p59;
	shl.b64 	%rd302, %rd301, 1;
	sub.s64 	%rd303, %rd302, %rd23;
	mov.b64 	%fd184, %rd303;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r436}, %fd184;
	}
	setp.lt.s32 	%p60, %r436, 0;
	selp.b64 	%rd304, %rd302, %rd303, %p60;
	shl.b64 	%rd305, %rd304, 1;
	sub.s64 	%rd306, %rd305, %rd23;
	mov.b64 	%fd185, %rd306;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r437}, %fd185;
	}
	setp.lt.s32 	%p61, %r437, 0;
	selp.b64 	%rd1101, %rd305, %rd306, %p61;
	shl.b64 	%rd1098, %rd1101, 1;
	add.s32 	%r1151, %r1151, -4;
	add.s32 	%r1150, %r1150, -1;
	setp.ne.s32 	%p62, %r1150, 0;
	@%p62 bra 	$L__BB0_36;

$L__BB0_37:
	setp.lt.u32 	%p63, %r49, 12;
	@%p63 bra 	$L__BB0_39;

$L__BB0_38:
	sub.s64 	%rd307, %rd1098, %rd23;
	mov.b64 	%fd186, %rd307;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r438}, %fd186;
	}
	setp.lt.s32 	%p64, %r438, 0;
	selp.b64 	%rd308, %rd1098, %rd307, %p64;
	shl.b64 	%rd309, %rd308, 1;
	sub.s64 	%rd310, %rd309, %rd23;
	mov.b64 	%fd187, %rd310;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r439}, %fd187;
	}
	setp.lt.s32 	%p65, %r439, 0;
	selp.b64 	%rd311, %rd309, %rd310, %p65;
	shl.b64 	%rd312, %rd311, 1;
	sub.s64 	%rd313, %rd312, %rd23;
	mov.b64 	%fd188, %rd313;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r440}, %fd188;
	}
	setp.lt.s32 	%p66, %r440, 0;
	selp.b64 	%rd314, %rd312, %rd313, %p66;
	shl.b64 	%rd315, %rd314, 1;
	sub.s64 	%rd316, %rd315, %rd23;
	mov.b64 	%fd189, %rd316;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r441}, %fd189;
	}
	setp.lt.s32 	%p67, %r441, 0;
	selp.b64 	%rd317, %rd315, %rd316, %p67;
	shl.b64 	%rd318, %rd317, 1;
	sub.s64 	%rd319, %rd318, %rd23;
	mov.b64 	%fd190, %rd319;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r442}, %fd190;
	}
	setp.lt.s32 	%p68, %r442, 0;
	selp.b64 	%rd320, %rd318, %rd319, %p68;
	shl.b64 	%rd321, %rd320, 1;
	sub.s64 	%rd322, %rd321, %rd23;
	mov.b64 	%fd191, %rd322;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r443}, %fd191;
	}
	setp.lt.s32 	%p69, %r443, 0;
	selp.b64 	%rd323, %rd321, %rd322, %p69;
	shl.b64 	%rd324, %rd323, 1;
	sub.s64 	%rd325, %rd324, %rd23;
	mov.b64 	%fd192, %rd325;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r444}, %fd192;
	}
	setp.lt.s32 	%p70, %r444, 0;
	selp.b64 	%rd326, %rd324, %rd325, %p70;
	shl.b64 	%rd327, %rd326, 1;
	sub.s64 	%rd328, %rd327, %rd23;
	mov.b64 	%fd193, %rd328;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r445}, %fd193;
	}
	setp.lt.s32 	%p71, %r445, 0;
	selp.b64 	%rd329, %rd327, %rd328, %p71;
	shl.b64 	%rd330, %rd329, 1;
	sub.s64 	%rd331, %rd330, %rd23;
	mov.b64 	%fd194, %rd331;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r446}, %fd194;
	}
	setp.lt.s32 	%p72, %r446, 0;
	selp.b64 	%rd332, %rd330, %rd331, %p72;
	shl.b64 	%rd333, %rd332, 1;
	sub.s64 	%rd334, %rd333, %rd23;
	mov.b64 	%fd195, %rd334;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r447}, %fd195;
	}
	setp.lt.s32 	%p73, %r447, 0;
	selp.b64 	%rd335, %rd333, %rd334, %p73;
	shl.b64 	%rd336, %rd335, 1;
	sub.s64 	%rd337, %rd336, %rd23;
	mov.b64 	%fd196, %rd337;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r448}, %fd196;
	}
	setp.lt.s32 	%p74, %r448, 0;
	selp.b64 	%rd338, %rd336, %rd337, %p74;
	shl.b64 	%rd339, %rd338, 1;
	sub.s64 	%rd340, %rd339, %rd23;
	mov.b64 	%fd197, %rd340;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r449}, %fd197;
	}
	setp.lt.s32 	%p75, %r449, 0;
	selp.b64 	%rd341, %rd339, %rd340, %p75;
	shl.b64 	%rd342, %rd341, 1;
	sub.s64 	%rd343, %rd342, %rd23;
	mov.b64 	%fd198, %rd343;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r450}, %fd198;
	}
	setp.lt.s32 	%p76, %r450, 0;
	selp.b64 	%rd344, %rd342, %rd343, %p76;
	shl.b64 	%rd345, %rd344, 1;
	sub.s64 	%rd346, %rd345, %rd23;
	mov.b64 	%fd199, %rd346;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r451}, %fd199;
	}
	setp.lt.s32 	%p77, %r451, 0;
	selp.b64 	%rd347, %rd345, %rd346, %p77;
	shl.b64 	%rd348, %rd347, 1;
	sub.s64 	%rd349, %rd348, %rd23;
	mov.b64 	%fd200, %rd349;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r452}, %fd200;
	}
	setp.lt.s32 	%p78, %r452, 0;
	selp.b64 	%rd350, %rd348, %rd349, %p78;
	shl.b64 	%rd351, %rd350, 1;
	sub.s64 	%rd352, %rd351, %rd23;
	mov.b64 	%fd201, %rd352;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r453}, %fd201;
	}
	setp.lt.s32 	%p79, %r453, 0;
	selp.b64 	%rd1101, %rd351, %rd352, %p79;
	shl.b64 	%rd1098, %rd1101, 1;
	add.s32 	%r57, %r1151, -16;
	setp.gt.s32 	%p80, %r1151, 15;
	mov.u32 	%r1151, %r57;
	@%p80 bra 	$L__BB0_38;

$L__BB0_39:
	and.b64  	%rd38, %rd1101, 9223372036854775807;
	setp.eq.s64 	%p81, %rd38, 0;
	mov.f64 	%fd520, 0d0000000000000000;
	@%p81 bra 	$L__BB0_41;

	mov.b64 	%fd203, %rd38;
	mul.f64 	%fd204, %fd203, 0d4350000000000000;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r454}, %fd204;
	}
	shr.u32 	%r455, %r454, 20;
	mov.u32 	%r456, 55;
	sub.s32 	%r457, %r456, %r455;
	sub.s32 	%r458, %r1145, %r457;
	shl.b64 	%rd353, %rd38, %r457;
	setp.lt.s32 	%p82, %r458, 1;
	mov.u32 	%r459, 1;
	sub.s32 	%r460, %r459, %r458;
	shr.u64 	%rd354, %rd353, %r460;
	add.s32 	%r461, %r458, -1;
	cvt.u64.u32 	%rd355, %r461;
	shl.b64 	%rd356, %rd355, 52;
	add.s64 	%rd357, %rd356, %rd353;
	selp.b64 	%rd358, %rd354, %rd357, %p82;
	mov.b64 	%fd520, %rd358;

$L__BB0_41:
	and.b32  	%r462, %r34, -2147483648;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r463}, %fd520;
	}
	or.b32  	%r464, %r463, %r462;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r465, %temp}, %fd520;
	}
	mov.b64 	%fd521, {%r465, %r464};
	bra.uni 	$L__BB0_45;

$L__BB0_43:
	mov.f64 	%fd205, 0d3FF0000000000000;
	add.rn.f64 	%fd521, %fd15, %fd205;

$L__BB0_45:
	abs.f64 	%fd207, %fd521;
	mul.f64 	%fd208, %fd207, 0d41EFFFFFFFE00000;
	cvt.rzi.u32.f64 	%r468, %fd208;
	mul.wide.u32 	%rd359, %r468, 795364315;
	shr.u64 	%rd360, %rd359, 32;
	cvt.u32.u64 	%r469, %rd360;
	sub.s32 	%r470, %r468, %r469;
	shr.u32 	%r471, %r470, 1;
	add.s32 	%r472, %r471, %r469;
	shr.u32 	%r473, %r472, 4;
	mul.lo.s32 	%r474, %r473, 27;
	sub.s32 	%r58, %r468, %r474;
	mov.u32 	%r475, 1060126512;
	mov.u32 	%r476, 1026899248;
	st.local.v4.u32 	[%rd2], {%r476, %r475, %r4, %r5};
	mov.f64 	%fd526, 0d3FF0000000000000;
	mov.u32 	%r1154, 16;
	mov.u32 	%r1153, 15;

$L__BB0_46:
	mov.u32 	%r60, %r1154;
	mov.u32 	%r1154, %r1153;
	setp.gt.u32 	%p527, %r6, 2146435071;
	cvt.s64.s32 	%rd361, %r1154;
	add.s64 	%rd362, %rd2, %rd361;
	ld.local.s8 	%rs12, [%rd362];
	cvt.rn.f64.s16 	%fd209, %rs12;
	mov.f64 	%fd210, 0d3FF1FB9C7406BB70;
	div.rn.f64 	%fd211, %fd210, %fd526;
	mul.f64 	%fd212, %fd211, %fd209;
	mul.f64 	%fd213, %fd212, 0d400921FB54442EEA;
	cvt.rn.f64.s32 	%fd214, %r60;
	fma.rn.f64 	%fd27, %fd214, 0d400921FB54442EEA, %fd213;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r61}, %fd27;
	}
	and.b32  	%r477, %r61, 2147483647;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r478, %temp}, %fd27;
	}
	mov.b64 	%fd523, {%r478, %r477};
	setp.gt.u32 	%p87, %r477, 2146435071;
	or.pred  	%p89, %p87, %p527;
	@%p89 bra 	$L__BB0_63;
	bra.uni 	$L__BB0_47;

$L__BB0_63:
	setp.le.f64 	%p124, %fd528, 0d7FF0000000000000;
	setp.le.f64 	%p125, %fd523, 0d7FF0000000000000;
	and.pred  	%p126, %p125, %p124;
	@%p126 bra 	$L__BB0_65;
	bra.uni 	$L__BB0_64;

$L__BB0_65:
	setp.eq.f64 	%p127, %fd523, 0d7FF0000000000000;
	selp.f64 	%fd526, 0dFFF8000000000000, %fd27, %p127;
	bra.uni 	$L__BB0_66;

$L__BB0_47:
	setp.eq.f64 	%p90, %fd528, 0d0000000000000000;
	mov.f64 	%fd526, 0dFFF8000000000000;
	@%p90 bra 	$L__BB0_66;

	setp.ltu.f64 	%p91, %fd523, %fd528;
	mov.f64 	%fd526, %fd27;
	@%p91 bra 	$L__BB0_66;

	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r479}, %fd523;
	}
	shr.u32 	%r1155, %r479, 20;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r480}, %fd528;
	}
	shr.u32 	%r1156, %r480, 20;
	setp.ne.s32 	%p92, %r1155, 0;
	@%p92 bra 	$L__BB0_51;

	mul.f64 	%fd523, %fd523, 0d4350000000000000;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r481}, %fd523;
	}
	shr.u32 	%r482, %r481, 20;
	add.s32 	%r1155, %r482, -54;

$L__BB0_51:
	setp.ne.s32 	%p93, %r1156, 0;
	mov.f64 	%fd524, %fd528;
	@%p93 bra 	$L__BB0_53;

	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r483}, %fd2;
	}
	shr.u32 	%r484, %r483, 20;
	add.s32 	%r1156, %r484, -54;
	mov.f64 	%fd524, %fd2;

$L__BB0_53:
	mov.b64 	%rd364, %fd523;
	and.b64  	%rd365, %rd364, 4503599627370495;
	or.b64  	%rd1106, %rd365, 4503599627370496;
	mov.b64 	%rd366, %fd524;
	and.b64  	%rd367, %rd366, 4503599627370495;
	or.b64  	%rd40, %rd367, 4503599627370496;
	sub.s32 	%r1162, %r1155, %r1156;
	not.b32 	%r485, %r1155;
	add.s32 	%r486, %r1156, %r485;
	max.s32 	%r487, %r486, -1;
	add.s32 	%r69, %r487, %r1155;
	mov.u32 	%r488, 2;
	sub.s32 	%r489, %r488, %r1156;
	add.s32 	%r490, %r489, %r69;
	and.b32  	%r1158, %r490, 3;
	setp.eq.s32 	%p94, %r1158, 0;
	@%p94 bra 	$L__BB0_55;

$L__BB0_54:
	.pragma "nounroll";
	sub.s64 	%rd368, %rd1106, %rd40;
	mov.b64 	%fd216, %rd368;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r491}, %fd216;
	}
	setp.lt.s32 	%p95, %r491, 0;
	selp.b64 	%rd1109, %rd1106, %rd368, %p95;
	shl.b64 	%rd1106, %rd1109, 1;
	add.s32 	%r1162, %r1162, -1;
	add.s32 	%r1158, %r1158, -1;
	setp.ne.s32 	%p96, %r1158, 0;
	@%p96 bra 	$L__BB0_54;

$L__BB0_55:
	mov.u32 	%r492, 1;
	sub.s32 	%r493, %r492, %r1156;
	add.s32 	%r494, %r493, %r69;
	setp.lt.u32 	%p97, %r494, 3;
	@%p97 bra 	$L__BB0_60;

	not.b32 	%r495, %r1162;
	max.s32 	%r496, %r495, -4;
	add.s32 	%r497, %r1162, %r496;
	add.s32 	%r76, %r497, 4;
	shr.u32 	%r498, %r76, 2;
	add.s32 	%r499, %r498, 1;
	and.b32  	%r1161, %r499, 3;
	setp.eq.s32 	%p98, %r1161, 0;
	@%p98 bra 	$L__BB0_58;

$L__BB0_57:
	.pragma "nounroll";
	sub.s64 	%rd370, %rd1106, %rd40;
	mov.b64 	%fd217, %rd370;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r500}, %fd217;
	}
	setp.lt.s32 	%p99, %r500, 0;
	selp.b64 	%rd371, %rd1106, %rd370, %p99;
	shl.b64 	%rd372, %rd371, 1;
	sub.s64 	%rd373, %rd372, %rd40;
	mov.b64 	%fd218, %rd373;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r501}, %fd218;
	}
	setp.lt.s32 	%p100, %r501, 0;
	selp.b64 	%rd374, %rd372, %rd373, %p100;
	shl.b64 	%rd375, %rd374, 1;
	sub.s64 	%rd376, %rd375, %rd40;
	mov.b64 	%fd219, %rd376;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r502}, %fd219;
	}
	setp.lt.s32 	%p101, %r502, 0;
	selp.b64 	%rd377, %rd375, %rd376, %p101;
	shl.b64 	%rd378, %rd377, 1;
	sub.s64 	%rd379, %rd378, %rd40;
	mov.b64 	%fd220, %rd379;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r503}, %fd220;
	}
	setp.lt.s32 	%p102, %r503, 0;
	selp.b64 	%rd1109, %rd378, %rd379, %p102;
	shl.b64 	%rd1106, %rd1109, 1;
	add.s32 	%r1162, %r1162, -4;
	add.s32 	%r1161, %r1161, -1;
	setp.ne.s32 	%p103, %r1161, 0;
	@%p103 bra 	$L__BB0_57;

$L__BB0_58:
	setp.lt.u32 	%p104, %r76, 12;
	@%p104 bra 	$L__BB0_60;

$L__BB0_59:
	sub.s64 	%rd380, %rd1106, %rd40;
	mov.b64 	%fd221, %rd380;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r504}, %fd221;
	}
	setp.lt.s32 	%p105, %r504, 0;
	selp.b64 	%rd381, %rd1106, %rd380, %p105;
	shl.b64 	%rd382, %rd381, 1;
	sub.s64 	%rd383, %rd382, %rd40;
	mov.b64 	%fd222, %rd383;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r505}, %fd222;
	}
	setp.lt.s32 	%p106, %r505, 0;
	selp.b64 	%rd384, %rd382, %rd383, %p106;
	shl.b64 	%rd385, %rd384, 1;
	sub.s64 	%rd386, %rd385, %rd40;
	mov.b64 	%fd223, %rd386;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r506}, %fd223;
	}
	setp.lt.s32 	%p107, %r506, 0;
	selp.b64 	%rd387, %rd385, %rd386, %p107;
	shl.b64 	%rd388, %rd387, 1;
	sub.s64 	%rd389, %rd388, %rd40;
	mov.b64 	%fd224, %rd389;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r507}, %fd224;
	}
	setp.lt.s32 	%p108, %r507, 0;
	selp.b64 	%rd390, %rd388, %rd389, %p108;
	shl.b64 	%rd391, %rd390, 1;
	sub.s64 	%rd392, %rd391, %rd40;
	mov.b64 	%fd225, %rd392;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r508}, %fd225;
	}
	setp.lt.s32 	%p109, %r508, 0;
	selp.b64 	%rd393, %rd391, %rd392, %p109;
	shl.b64 	%rd394, %rd393, 1;
	sub.s64 	%rd395, %rd394, %rd40;
	mov.b64 	%fd226, %rd395;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r509}, %fd226;
	}
	setp.lt.s32 	%p110, %r509, 0;
	selp.b64 	%rd396, %rd394, %rd395, %p110;
	shl.b64 	%rd397, %rd396, 1;
	sub.s64 	%rd398, %rd397, %rd40;
	mov.b64 	%fd227, %rd398;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r510}, %fd227;
	}
	setp.lt.s32 	%p111, %r510, 0;
	selp.b64 	%rd399, %rd397, %rd398, %p111;
	shl.b64 	%rd400, %rd399, 1;
	sub.s64 	%rd401, %rd400, %rd40;
	mov.b64 	%fd228, %rd401;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r511}, %fd228;
	}
	setp.lt.s32 	%p112, %r511, 0;
	selp.b64 	%rd402, %rd400, %rd401, %p112;
	shl.b64 	%rd403, %rd402, 1;
	sub.s64 	%rd404, %rd403, %rd40;
	mov.b64 	%fd229, %rd404;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r512}, %fd229;
	}
	setp.lt.s32 	%p113, %r512, 0;
	selp.b64 	%rd405, %rd403, %rd404, %p113;
	shl.b64 	%rd406, %rd405, 1;
	sub.s64 	%rd407, %rd406, %rd40;
	mov.b64 	%fd230, %rd407;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r513}, %fd230;
	}
	setp.lt.s32 	%p114, %r513, 0;
	selp.b64 	%rd408, %rd406, %rd407, %p114;
	shl.b64 	%rd409, %rd408, 1;
	sub.s64 	%rd410, %rd409, %rd40;
	mov.b64 	%fd231, %rd410;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r514}, %fd231;
	}
	setp.lt.s32 	%p115, %r514, 0;
	selp.b64 	%rd411, %rd409, %rd410, %p115;
	shl.b64 	%rd412, %rd411, 1;
	sub.s64 	%rd413, %rd412, %rd40;
	mov.b64 	%fd232, %rd413;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r515}, %fd232;
	}
	setp.lt.s32 	%p116, %r515, 0;
	selp.b64 	%rd414, %rd412, %rd413, %p116;
	shl.b64 	%rd415, %rd414, 1;
	sub.s64 	%rd416, %rd415, %rd40;
	mov.b64 	%fd233, %rd416;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r516}, %fd233;
	}
	setp.lt.s32 	%p117, %r516, 0;
	selp.b64 	%rd417, %rd415, %rd416, %p117;
	shl.b64 	%rd418, %rd417, 1;
	sub.s64 	%rd419, %rd418, %rd40;
	mov.b64 	%fd234, %rd419;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r517}, %fd234;
	}
	setp.lt.s32 	%p118, %r517, 0;
	selp.b64 	%rd420, %rd418, %rd419, %p118;
	shl.b64 	%rd421, %rd420, 1;
	sub.s64 	%rd422, %rd421, %rd40;
	mov.b64 	%fd235, %rd422;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r518}, %fd235;
	}
	setp.lt.s32 	%p119, %r518, 0;
	selp.b64 	%rd423, %rd421, %rd422, %p119;
	shl.b64 	%rd424, %rd423, 1;
	sub.s64 	%rd425, %rd424, %rd40;
	mov.b64 	%fd236, %rd425;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r519}, %fd236;
	}
	setp.lt.s32 	%p120, %r519, 0;
	selp.b64 	%rd1109, %rd424, %rd425, %p120;
	shl.b64 	%rd1106, %rd1109, 1;
	add.s32 	%r84, %r1162, -16;
	setp.gt.s32 	%p121, %r1162, 15;
	mov.u32 	%r1162, %r84;
	@%p121 bra 	$L__BB0_59;

$L__BB0_60:
	and.b64  	%rd55, %rd1109, 9223372036854775807;
	setp.eq.s64 	%p122, %rd55, 0;
	mov.f64 	%fd525, 0d0000000000000000;
	@%p122 bra 	$L__BB0_62;

	mov.b64 	%fd238, %rd55;
	mul.f64 	%fd239, %fd238, 0d4350000000000000;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r520}, %fd239;
	}
	shr.u32 	%r521, %r520, 20;
	mov.u32 	%r522, 55;
	sub.s32 	%r523, %r522, %r521;
	sub.s32 	%r524, %r1156, %r523;
	shl.b64 	%rd426, %rd55, %r523;
	setp.lt.s32 	%p123, %r524, 1;
	mov.u32 	%r525, 1;
	sub.s32 	%r526, %r525, %r524;
	shr.u64 	%rd427, %rd426, %r526;
	add.s32 	%r527, %r524, -1;
	cvt.u64.u32 	%rd428, %r527;
	shl.b64 	%rd429, %rd428, 52;
	add.s64 	%rd430, %rd429, %rd426;
	selp.b64 	%rd431, %rd427, %rd430, %p123;
	mov.b64 	%fd525, %rd431;

$L__BB0_62:
	and.b32  	%r528, %r61, -2147483648;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r529}, %fd525;
	}
	or.b32  	%r530, %r529, %r528;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r531, %temp}, %fd525;
	}
	mov.b64 	%fd526, {%r531, %r530};
	bra.uni 	$L__BB0_66;

$L__BB0_64:
	mov.f64 	%fd240, 0d3FF0000000000000;
	add.rn.f64 	%fd526, %fd27, %fd240;

$L__BB0_66:
	add.s32 	%r1153, %r1154, -1;
	setp.gt.s32 	%p128, %r1154, 0;
	@%p128 bra 	$L__BB0_46;

	setp.gt.u32 	%p528, %r6, 2146435071;
	fma.rn.f64 	%fd38, %fd526, 0d3FFB96C889463A4C, 0d4001135C51C7F716;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r86}, %fd38;
	}
	and.b32  	%r532, %r86, 2147483647;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r533, %temp}, %fd38;
	}
	mov.b64 	%fd527, {%r533, %r532};
	setp.gt.u32 	%p130, %r532, 2146435071;
	or.pred  	%p131, %p130, %p528;
	@%p131 bra 	$L__BB0_84;
	bra.uni 	$L__BB0_68;

$L__BB0_84:
	setp.le.f64 	%p166, %fd528, 0d7FF0000000000000;
	setp.le.f64 	%p167, %fd527, 0d7FF0000000000000;
	and.pred  	%p168, %p167, %p166;
	@%p168 bra 	$L__BB0_86;
	bra.uni 	$L__BB0_85;

$L__BB0_86:
	setp.eq.f64 	%p169, %fd527, 0d7FF0000000000000;
	selp.f64 	%fd530, 0dFFF8000000000000, %fd38, %p169;
	bra.uni 	$L__BB0_87;

$L__BB0_68:
	setp.eq.f64 	%p132, %fd528, 0d0000000000000000;
	mov.f64 	%fd530, 0dFFF8000000000000;
	@%p132 bra 	$L__BB0_87;

	setp.ltu.f64 	%p133, %fd527, %fd528;
	mov.f64 	%fd530, %fd38;
	@%p133 bra 	$L__BB0_87;

	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r534}, %fd527;
	}
	shr.u32 	%r1164, %r534, 20;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r535}, %fd528;
	}
	shr.u32 	%r1165, %r535, 20;
	setp.ne.s32 	%p134, %r1164, 0;
	@%p134 bra 	$L__BB0_72;

	mul.f64 	%fd527, %fd527, 0d4350000000000000;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r536}, %fd527;
	}
	shr.u32 	%r537, %r536, 20;
	add.s32 	%r1164, %r537, -54;

$L__BB0_72:
	setp.ne.s32 	%p135, %r1165, 0;
	@%p135 bra 	$L__BB0_74;

	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r538}, %fd2;
	}
	shr.u32 	%r539, %r538, 20;
	add.s32 	%r1165, %r539, -54;
	mov.f64 	%fd528, %fd2;

$L__BB0_74:
	mov.b64 	%rd433, %fd527;
	and.b64  	%rd434, %rd433, 4503599627370495;
	or.b64  	%rd1114, %rd434, 4503599627370496;
	mov.b64 	%rd435, %fd528;
	and.b64  	%rd436, %rd435, 4503599627370495;
	or.b64  	%rd57, %rd436, 4503599627370496;
	sub.s32 	%r1171, %r1164, %r1165;
	not.b32 	%r540, %r1164;
	add.s32 	%r541, %r1165, %r540;
	max.s32 	%r542, %r541, -1;
	add.s32 	%r94, %r542, %r1164;
	mov.u32 	%r543, 2;
	sub.s32 	%r544, %r543, %r1165;
	add.s32 	%r545, %r544, %r94;
	and.b32  	%r1167, %r545, 3;
	setp.eq.s32 	%p136, %r1167, 0;
	@%p136 bra 	$L__BB0_76;

$L__BB0_75:
	.pragma "nounroll";
	sub.s64 	%rd437, %rd1114, %rd57;
	mov.b64 	%fd242, %rd437;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r546}, %fd242;
	}
	setp.lt.s32 	%p137, %r546, 0;
	selp.b64 	%rd1117, %rd1114, %rd437, %p137;
	shl.b64 	%rd1114, %rd1117, 1;
	add.s32 	%r1171, %r1171, -1;
	add.s32 	%r1167, %r1167, -1;
	setp.ne.s32 	%p138, %r1167, 0;
	@%p138 bra 	$L__BB0_75;

$L__BB0_76:
	mov.u32 	%r547, 1;
	sub.s32 	%r548, %r547, %r1165;
	add.s32 	%r549, %r548, %r94;
	setp.lt.u32 	%p139, %r549, 3;
	@%p139 bra 	$L__BB0_81;

	not.b32 	%r550, %r1171;
	max.s32 	%r551, %r550, -4;
	add.s32 	%r552, %r1171, %r551;
	add.s32 	%r101, %r552, 4;
	shr.u32 	%r553, %r101, 2;
	add.s32 	%r554, %r553, 1;
	and.b32  	%r1170, %r554, 3;
	setp.eq.s32 	%p140, %r1170, 0;
	@%p140 bra 	$L__BB0_79;

$L__BB0_78:
	.pragma "nounroll";
	sub.s64 	%rd439, %rd1114, %rd57;
	mov.b64 	%fd243, %rd439;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r555}, %fd243;
	}
	setp.lt.s32 	%p141, %r555, 0;
	selp.b64 	%rd440, %rd1114, %rd439, %p141;
	shl.b64 	%rd441, %rd440, 1;
	sub.s64 	%rd442, %rd441, %rd57;
	mov.b64 	%fd244, %rd442;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r556}, %fd244;
	}
	setp.lt.s32 	%p142, %r556, 0;
	selp.b64 	%rd443, %rd441, %rd442, %p142;
	shl.b64 	%rd444, %rd443, 1;
	sub.s64 	%rd445, %rd444, %rd57;
	mov.b64 	%fd245, %rd445;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r557}, %fd245;
	}
	setp.lt.s32 	%p143, %r557, 0;
	selp.b64 	%rd446, %rd444, %rd445, %p143;
	shl.b64 	%rd447, %rd446, 1;
	sub.s64 	%rd448, %rd447, %rd57;
	mov.b64 	%fd246, %rd448;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r558}, %fd246;
	}
	setp.lt.s32 	%p144, %r558, 0;
	selp.b64 	%rd1117, %rd447, %rd448, %p144;
	shl.b64 	%rd1114, %rd1117, 1;
	add.s32 	%r1171, %r1171, -4;
	add.s32 	%r1170, %r1170, -1;
	setp.ne.s32 	%p145, %r1170, 0;
	@%p145 bra 	$L__BB0_78;

$L__BB0_79:
	setp.lt.u32 	%p146, %r101, 12;
	@%p146 bra 	$L__BB0_81;

$L__BB0_80:
	sub.s64 	%rd449, %rd1114, %rd57;
	mov.b64 	%fd247, %rd449;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r559}, %fd247;
	}
	setp.lt.s32 	%p147, %r559, 0;
	selp.b64 	%rd450, %rd1114, %rd449, %p147;
	shl.b64 	%rd451, %rd450, 1;
	sub.s64 	%rd452, %rd451, %rd57;
	mov.b64 	%fd248, %rd452;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r560}, %fd248;
	}
	setp.lt.s32 	%p148, %r560, 0;
	selp.b64 	%rd453, %rd451, %rd452, %p148;
	shl.b64 	%rd454, %rd453, 1;
	sub.s64 	%rd455, %rd454, %rd57;
	mov.b64 	%fd249, %rd455;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r561}, %fd249;
	}
	setp.lt.s32 	%p149, %r561, 0;
	selp.b64 	%rd456, %rd454, %rd455, %p149;
	shl.b64 	%rd457, %rd456, 1;
	sub.s64 	%rd458, %rd457, %rd57;
	mov.b64 	%fd250, %rd458;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r562}, %fd250;
	}
	setp.lt.s32 	%p150, %r562, 0;
	selp.b64 	%rd459, %rd457, %rd458, %p150;
	shl.b64 	%rd460, %rd459, 1;
	sub.s64 	%rd461, %rd460, %rd57;
	mov.b64 	%fd251, %rd461;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r563}, %fd251;
	}
	setp.lt.s32 	%p151, %r563, 0;
	selp.b64 	%rd462, %rd460, %rd461, %p151;
	shl.b64 	%rd463, %rd462, 1;
	sub.s64 	%rd464, %rd463, %rd57;
	mov.b64 	%fd252, %rd464;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r564}, %fd252;
	}
	setp.lt.s32 	%p152, %r564, 0;
	selp.b64 	%rd465, %rd463, %rd464, %p152;
	shl.b64 	%rd466, %rd465, 1;
	sub.s64 	%rd467, %rd466, %rd57;
	mov.b64 	%fd253, %rd467;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r565}, %fd253;
	}
	setp.lt.s32 	%p153, %r565, 0;
	selp.b64 	%rd468, %rd466, %rd467, %p153;
	shl.b64 	%rd469, %rd468, 1;
	sub.s64 	%rd470, %rd469, %rd57;
	mov.b64 	%fd254, %rd470;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r566}, %fd254;
	}
	setp.lt.s32 	%p154, %r566, 0;
	selp.b64 	%rd471, %rd469, %rd470, %p154;
	shl.b64 	%rd472, %rd471, 1;
	sub.s64 	%rd473, %rd472, %rd57;
	mov.b64 	%fd255, %rd473;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r567}, %fd255;
	}
	setp.lt.s32 	%p155, %r567, 0;
	selp.b64 	%rd474, %rd472, %rd473, %p155;
	shl.b64 	%rd475, %rd474, 1;
	sub.s64 	%rd476, %rd475, %rd57;
	mov.b64 	%fd256, %rd476;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r568}, %fd256;
	}
	setp.lt.s32 	%p156, %r568, 0;
	selp.b64 	%rd477, %rd475, %rd476, %p156;
	shl.b64 	%rd478, %rd477, 1;
	sub.s64 	%rd479, %rd478, %rd57;
	mov.b64 	%fd257, %rd479;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r569}, %fd257;
	}
	setp.lt.s32 	%p157, %r569, 0;
	selp.b64 	%rd480, %rd478, %rd479, %p157;
	shl.b64 	%rd481, %rd480, 1;
	sub.s64 	%rd482, %rd481, %rd57;
	mov.b64 	%fd258, %rd482;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r570}, %fd258;
	}
	setp.lt.s32 	%p158, %r570, 0;
	selp.b64 	%rd483, %rd481, %rd482, %p158;
	shl.b64 	%rd484, %rd483, 1;
	sub.s64 	%rd485, %rd484, %rd57;
	mov.b64 	%fd259, %rd485;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r571}, %fd259;
	}
	setp.lt.s32 	%p159, %r571, 0;
	selp.b64 	%rd486, %rd484, %rd485, %p159;
	shl.b64 	%rd487, %rd486, 1;
	sub.s64 	%rd488, %rd487, %rd57;
	mov.b64 	%fd260, %rd488;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r572}, %fd260;
	}
	setp.lt.s32 	%p160, %r572, 0;
	selp.b64 	%rd489, %rd487, %rd488, %p160;
	shl.b64 	%rd490, %rd489, 1;
	sub.s64 	%rd491, %rd490, %rd57;
	mov.b64 	%fd261, %rd491;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r573}, %fd261;
	}
	setp.lt.s32 	%p161, %r573, 0;
	selp.b64 	%rd492, %rd490, %rd491, %p161;
	shl.b64 	%rd493, %rd492, 1;
	sub.s64 	%rd494, %rd493, %rd57;
	mov.b64 	%fd262, %rd494;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r574}, %fd262;
	}
	setp.lt.s32 	%p162, %r574, 0;
	selp.b64 	%rd1117, %rd493, %rd494, %p162;
	shl.b64 	%rd1114, %rd1117, 1;
	add.s32 	%r109, %r1171, -16;
	setp.gt.s32 	%p163, %r1171, 15;
	mov.u32 	%r1171, %r109;
	@%p163 bra 	$L__BB0_80;

$L__BB0_81:
	and.b64  	%rd72, %rd1117, 9223372036854775807;
	setp.eq.s64 	%p164, %rd72, 0;
	mov.f64 	%fd529, 0d0000000000000000;
	@%p164 bra 	$L__BB0_83;

	mov.b64 	%fd264, %rd72;
	mul.f64 	%fd265, %fd264, 0d4350000000000000;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r575}, %fd265;
	}
	shr.u32 	%r576, %r575, 20;
	mov.u32 	%r577, 55;
	sub.s32 	%r578, %r577, %r576;
	sub.s32 	%r579, %r1165, %r578;
	shl.b64 	%rd495, %rd72, %r578;
	setp.lt.s32 	%p165, %r579, 1;
	mov.u32 	%r580, 1;
	sub.s32 	%r581, %r580, %r579;
	shr.u64 	%rd496, %rd495, %r581;
	add.s32 	%r582, %r579, -1;
	cvt.u64.u32 	%rd497, %r582;
	shl.b64 	%rd498, %rd497, 52;
	add.s64 	%rd499, %rd498, %rd495;
	selp.b64 	%rd500, %rd496, %rd499, %p165;
	mov.b64 	%fd529, %rd500;

$L__BB0_83:
	and.b32  	%r583, %r86, -2147483648;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r584}, %fd529;
	}
	or.b32  	%r585, %r584, %r583;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r586, %temp}, %fd529;
	}
	mov.b64 	%fd530, {%r586, %r585};
	bra.uni 	$L__BB0_87;

$L__BB0_85:
	mov.f64 	%fd266, 0d3FF0000000000000;
	add.rn.f64 	%fd530, %fd38, %fd266;

$L__BB0_87:
	ld.param.u32 	%r1122, [_Z17find_seeds_kernelmj12FilterParamsPmPVi_param_2+4];
	abs.f64 	%fd267, %fd530;
	mul.f64 	%fd268, %fd267, 0d41EFFFFFFFE00000;
	cvt.rzi.u32.f64 	%r587, %fd268;
	mul.wide.u32 	%rd501, %r587, 795364315;
	shr.u64 	%rd502, %rd501, 32;
	cvt.u32.u64 	%r588, %rd502;
	sub.s32 	%r589, %r587, %r588;
	shr.u32 	%r590, %r589, 1;
	add.s32 	%r591, %r590, %r588;
	shr.u32 	%r592, %r591, 4;
	mul.lo.s32 	%r593, %r592, 27;
	sub.s32 	%r110, %r587, %r593;
	setp.eq.s32 	%p170, %r1122, -1;
	@%p170 bra 	$L__BB0_91;

	ld.param.u32 	%r1129, [_Z17find_seeds_kernelmj12FilterParamsPmPVi_param_2];
	ld.param.u32 	%r1123, [_Z17find_seeds_kernelmj12FilterParamsPmPVi_param_2+4];
	setp.eq.s32 	%p171, %r1129, %r1123;
	@%p171 bra 	$L__BB0_90;
	bra.uni 	$L__BB0_89;

$L__BB0_90:
	ld.param.u32 	%r1131, [_Z17find_seeds_kernelmj12FilterParamsPmPVi_param_2];
	setp.eq.s32 	%p179, %r110, %r1131;
	setp.eq.s32 	%p180, %r58, %r1131;
	and.pred  	%p181, %p180, %p179;
	@%p181 bra 	$L__BB0_92;
	bra.uni 	$L__BB0_269;

$L__BB0_91:
	ld.param.u32 	%r1132, [_Z17find_seeds_kernelmj12FilterParamsPmPVi_param_2];
	setp.ne.s32 	%p182, %r58, %r1132;
	setp.ne.s32 	%p183, %r110, %r1132;
	and.pred  	%p184, %p182, %p183;
	@%p184 bra 	$L__BB0_269;
	bra.uni 	$L__BB0_92;

$L__BB0_89:
	ld.param.u32 	%r1130, [_Z17find_seeds_kernelmj12FilterParamsPmPVi_param_2];
	ld.param.u32 	%r1124, [_Z17find_seeds_kernelmj12FilterParamsPmPVi_param_2+4];
	setp.eq.s32 	%p172, %r110, %r1130;
	setp.eq.s32 	%p173, %r58, %r1130;
	or.pred  	%p174, %p173, %p172;
	setp.eq.s32 	%p175, %r110, %r1124;
	setp.eq.s32 	%p176, %r58, %r1124;
	or.pred  	%p177, %p176, %p175;
	and.pred  	%p178, %p174, %p177;
	@%p178 bra 	$L__BB0_92;
	bra.uni 	$L__BB0_269;

$L__BB0_92:
	ld.param.u32 	%r1125, [_Z17find_seeds_kernelmj12FilterParamsPmPVi_param_2+8];
	setp.eq.s32 	%p185, %r1125, -1;
	@%p185 bra 	$L__BB0_136;

	shr.u64 	%rd503, %rd3, 56;
	cvt.u32.u64 	%r596, %rd3;
	shr.u64 	%rd504, %rd3, 48;
	cvt.u32.u64 	%r597, %rd504;
	shr.u64 	%rd505, %rd3, 40;
	shr.u64 	%rd506, %rd3, 32;
	cvt.u32.u64 	%r598, %rd506;
	shr.u64 	%rd507, %rd3, 24;
	shr.u32 	%r599, %r596, 16;
	mov.u32 	%r1174, 16;
	and.b32  	%r600, %r596, 255;
	cvt.u16.u64 	%rs13, %rd3;
	shr.u16 	%rs14, %rs13, 8;
	cvt.u32.u16 	%r601, %rs14;
	prmt.b32 	%r602, %r601, %r600, 30212;
	cvt.u32.u64 	%r603, %rd507;
	and.b32  	%r604, %r599, 255;
	prmt.b32 	%r605, %r603, %r604, 30212;
	cvt.u32.u64 	%r606, %rd505;
	and.b32  	%r607, %r598, 255;
	prmt.b32 	%r608, %r606, %r607, 30212;
	cvt.u32.u64 	%r609, %rd503;
	and.b32  	%r610, %r597, 255;
	prmt.b32 	%r611, %r609, %r610, 30212;
	mov.u16 	%rs15, 14641;
	mov.u16 	%rs16, 12593;
	mov.u16 	%rs17, 14897;
	mov.u16 	%rs18, 13621;
	prmt.b32 	%r612, %r611, %r608, 4180;
	prmt.b32 	%r613, %r605, %r602, 4180;
	mov.b32 	%r614, {%rs18, %rs17};
	mov.b32 	%r615, {%rs16, %rs15};
	st.local.v4.u32 	[%rd2], {%r615, %r614, %r613, %r612};
	mov.f64 	%fd535, 0d3FF0000000000000;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r616}, %fd535;
	}
	and.b32  	%r112, %r616, 2147483647;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r617, %temp}, %fd535;
	}
	mov.b64 	%fd537, {%r617, %r112};
	mul.f64 	%fd50, %fd537, 0d4350000000000000;
	mov.u32 	%r1173, 15;

$L__BB0_94:
	mov.u32 	%r114, %r1174;
	mov.u32 	%r1174, %r1173;
	cvt.s64.s32 	%rd508, %r1174;
	add.s64 	%rd509, %rd2, %rd508;
	ld.local.s8 	%rs19, [%rd509];
	cvt.rn.f64.s16 	%fd270, %rs19;
	mov.f64 	%fd271, 0d3FF1FB9C7406BB70;
	div.rn.f64 	%fd272, %fd271, %fd535;
	mul.f64 	%fd273, %fd272, %fd270;
	mul.f64 	%fd274, %fd273, 0d400921FB54442EEA;
	cvt.rn.f64.s32 	%fd275, %r114;
	fma.rn.f64 	%fd52, %fd275, 0d400921FB54442EEA, %fd274;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r115}, %fd52;
	}
	and.b32  	%r618, %r115, 2147483647;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r619, %temp}, %fd52;
	}
	mov.b64 	%fd532, {%r619, %r618};
	setp.gt.u32 	%p186, %r618, 2146435071;
	setp.gt.u32 	%p187, %r112, 2146435071;
	or.pred  	%p188, %p186, %p187;
	@%p188 bra 	$L__BB0_111;
	bra.uni 	$L__BB0_95;

$L__BB0_111:
	setp.le.f64 	%p223, %fd537, 0d7FF0000000000000;
	setp.le.f64 	%p224, %fd532, 0d7FF0000000000000;
	and.pred  	%p225, %p224, %p223;
	@%p225 bra 	$L__BB0_113;
	bra.uni 	$L__BB0_112;

$L__BB0_113:
	setp.eq.f64 	%p226, %fd532, 0d7FF0000000000000;
	selp.f64 	%fd535, 0dFFF8000000000000, %fd52, %p226;
	bra.uni 	$L__BB0_114;

$L__BB0_95:
	setp.eq.f64 	%p189, %fd537, 0d0000000000000000;
	mov.f64 	%fd535, 0dFFF8000000000000;
	@%p189 bra 	$L__BB0_114;

	setp.ltu.f64 	%p190, %fd532, %fd537;
	mov.f64 	%fd535, %fd52;
	@%p190 bra 	$L__BB0_114;

	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r620}, %fd532;
	}
	shr.u32 	%r1175, %r620, 20;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r621}, %fd537;
	}
	shr.u32 	%r1176, %r621, 20;
	setp.ne.s32 	%p191, %r1175, 0;
	@%p191 bra 	$L__BB0_99;

	mul.f64 	%fd532, %fd532, 0d4350000000000000;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r622}, %fd532;
	}
	shr.u32 	%r623, %r622, 20;
	add.s32 	%r1175, %r623, -54;

$L__BB0_99:
	setp.ne.s32 	%p192, %r1176, 0;
	mov.f64 	%fd533, %fd537;
	@%p192 bra 	$L__BB0_101;

	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r624}, %fd50;
	}
	shr.u32 	%r625, %r624, 20;
	add.s32 	%r1176, %r625, -54;
	mov.f64 	%fd533, %fd50;

$L__BB0_101:
	mov.b64 	%rd511, %fd532;
	and.b64  	%rd512, %rd511, 4503599627370495;
	or.b64  	%rd1122, %rd512, 4503599627370496;
	mov.b64 	%rd513, %fd533;
	and.b64  	%rd514, %rd513, 4503599627370495;
	or.b64  	%rd74, %rd514, 4503599627370496;
	sub.s32 	%r1182, %r1175, %r1176;
	not.b32 	%r626, %r1175;
	add.s32 	%r627, %r1176, %r626;
	max.s32 	%r628, %r627, -1;
	add.s32 	%r123, %r628, %r1175;
	mov.u32 	%r629, 2;
	sub.s32 	%r630, %r629, %r1176;
	add.s32 	%r631, %r630, %r123;
	and.b32  	%r1178, %r631, 3;
	setp.eq.s32 	%p193, %r1178, 0;
	@%p193 bra 	$L__BB0_103;

$L__BB0_102:
	.pragma "nounroll";
	sub.s64 	%rd515, %rd1122, %rd74;
	mov.b64 	%fd277, %rd515;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r632}, %fd277;
	}
	setp.lt.s32 	%p194, %r632, 0;
	selp.b64 	%rd1125, %rd1122, %rd515, %p194;
	shl.b64 	%rd1122, %rd1125, 1;
	add.s32 	%r1182, %r1182, -1;
	add.s32 	%r1178, %r1178, -1;
	setp.ne.s32 	%p195, %r1178, 0;
	@%p195 bra 	$L__BB0_102;

$L__BB0_103:
	mov.u32 	%r633, 1;
	sub.s32 	%r634, %r633, %r1176;
	add.s32 	%r635, %r634, %r123;
	setp.lt.u32 	%p196, %r635, 3;
	@%p196 bra 	$L__BB0_108;

	not.b32 	%r636, %r1182;
	max.s32 	%r637, %r636, -4;
	add.s32 	%r638, %r1182, %r637;
	add.s32 	%r130, %r638, 4;
	shr.u32 	%r639, %r130, 2;
	add.s32 	%r640, %r639, 1;
	and.b32  	%r1181, %r640, 3;
	setp.eq.s32 	%p197, %r1181, 0;
	@%p197 bra 	$L__BB0_106;

$L__BB0_105:
	.pragma "nounroll";
	sub.s64 	%rd517, %rd1122, %rd74;
	mov.b64 	%fd278, %rd517;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r641}, %fd278;
	}
	setp.lt.s32 	%p198, %r641, 0;
	selp.b64 	%rd518, %rd1122, %rd517, %p198;
	shl.b64 	%rd519, %rd518, 1;
	sub.s64 	%rd520, %rd519, %rd74;
	mov.b64 	%fd279, %rd520;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r642}, %fd279;
	}
	setp.lt.s32 	%p199, %r642, 0;
	selp.b64 	%rd521, %rd519, %rd520, %p199;
	shl.b64 	%rd522, %rd521, 1;
	sub.s64 	%rd523, %rd522, %rd74;
	mov.b64 	%fd280, %rd523;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r643}, %fd280;
	}
	setp.lt.s32 	%p200, %r643, 0;
	selp.b64 	%rd524, %rd522, %rd523, %p200;
	shl.b64 	%rd525, %rd524, 1;
	sub.s64 	%rd526, %rd525, %rd74;
	mov.b64 	%fd281, %rd526;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r644}, %fd281;
	}
	setp.lt.s32 	%p201, %r644, 0;
	selp.b64 	%rd1125, %rd525, %rd526, %p201;
	shl.b64 	%rd1122, %rd1125, 1;
	add.s32 	%r1182, %r1182, -4;
	add.s32 	%r1181, %r1181, -1;
	setp.ne.s32 	%p202, %r1181, 0;
	@%p202 bra 	$L__BB0_105;

$L__BB0_106:
	setp.lt.u32 	%p203, %r130, 12;
	@%p203 bra 	$L__BB0_108;

$L__BB0_107:
	sub.s64 	%rd527, %rd1122, %rd74;
	mov.b64 	%fd282, %rd527;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r645}, %fd282;
	}
	setp.lt.s32 	%p204, %r645, 0;
	selp.b64 	%rd528, %rd1122, %rd527, %p204;
	shl.b64 	%rd529, %rd528, 1;
	sub.s64 	%rd530, %rd529, %rd74;
	mov.b64 	%fd283, %rd530;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r646}, %fd283;
	}
	setp.lt.s32 	%p205, %r646, 0;
	selp.b64 	%rd531, %rd529, %rd530, %p205;
	shl.b64 	%rd532, %rd531, 1;
	sub.s64 	%rd533, %rd532, %rd74;
	mov.b64 	%fd284, %rd533;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r647}, %fd284;
	}
	setp.lt.s32 	%p206, %r647, 0;
	selp.b64 	%rd534, %rd532, %rd533, %p206;
	shl.b64 	%rd535, %rd534, 1;
	sub.s64 	%rd536, %rd535, %rd74;
	mov.b64 	%fd285, %rd536;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r648}, %fd285;
	}
	setp.lt.s32 	%p207, %r648, 0;
	selp.b64 	%rd537, %rd535, %rd536, %p207;
	shl.b64 	%rd538, %rd537, 1;
	sub.s64 	%rd539, %rd538, %rd74;
	mov.b64 	%fd286, %rd539;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r649}, %fd286;
	}
	setp.lt.s32 	%p208, %r649, 0;
	selp.b64 	%rd540, %rd538, %rd539, %p208;
	shl.b64 	%rd541, %rd540, 1;
	sub.s64 	%rd542, %rd541, %rd74;
	mov.b64 	%fd287, %rd542;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r650}, %fd287;
	}
	setp.lt.s32 	%p209, %r650, 0;
	selp.b64 	%rd543, %rd541, %rd542, %p209;
	shl.b64 	%rd544, %rd543, 1;
	sub.s64 	%rd545, %rd544, %rd74;
	mov.b64 	%fd288, %rd545;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r651}, %fd288;
	}
	setp.lt.s32 	%p210, %r651, 0;
	selp.b64 	%rd546, %rd544, %rd545, %p210;
	shl.b64 	%rd547, %rd546, 1;
	sub.s64 	%rd548, %rd547, %rd74;
	mov.b64 	%fd289, %rd548;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r652}, %fd289;
	}
	setp.lt.s32 	%p211, %r652, 0;
	selp.b64 	%rd549, %rd547, %rd548, %p211;
	shl.b64 	%rd550, %rd549, 1;
	sub.s64 	%rd551, %rd550, %rd74;
	mov.b64 	%fd290, %rd551;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r653}, %fd290;
	}
	setp.lt.s32 	%p212, %r653, 0;
	selp.b64 	%rd552, %rd550, %rd551, %p212;
	shl.b64 	%rd553, %rd552, 1;
	sub.s64 	%rd554, %rd553, %rd74;
	mov.b64 	%fd291, %rd554;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r654}, %fd291;
	}
	setp.lt.s32 	%p213, %r654, 0;
	selp.b64 	%rd555, %rd553, %rd554, %p213;
	shl.b64 	%rd556, %rd555, 1;
	sub.s64 	%rd557, %rd556, %rd74;
	mov.b64 	%fd292, %rd557;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r655}, %fd292;
	}
	setp.lt.s32 	%p214, %r655, 0;
	selp.b64 	%rd558, %rd556, %rd557, %p214;
	shl.b64 	%rd559, %rd558, 1;
	sub.s64 	%rd560, %rd559, %rd74;
	mov.b64 	%fd293, %rd560;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r656}, %fd293;
	}
	setp.lt.s32 	%p215, %r656, 0;
	selp.b64 	%rd561, %rd559, %rd560, %p215;
	shl.b64 	%rd562, %rd561, 1;
	sub.s64 	%rd563, %rd562, %rd74;
	mov.b64 	%fd294, %rd563;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r657}, %fd294;
	}
	setp.lt.s32 	%p216, %r657, 0;
	selp.b64 	%rd564, %rd562, %rd563, %p216;
	shl.b64 	%rd565, %rd564, 1;
	sub.s64 	%rd566, %rd565, %rd74;
	mov.b64 	%fd295, %rd566;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r658}, %fd295;
	}
	setp.lt.s32 	%p217, %r658, 0;
	selp.b64 	%rd567, %rd565, %rd566, %p217;
	shl.b64 	%rd568, %rd567, 1;
	sub.s64 	%rd569, %rd568, %rd74;
	mov.b64 	%fd296, %rd569;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r659}, %fd296;
	}
	setp.lt.s32 	%p218, %r659, 0;
	selp.b64 	%rd570, %rd568, %rd569, %p218;
	shl.b64 	%rd571, %rd570, 1;
	sub.s64 	%rd572, %rd571, %rd74;
	mov.b64 	%fd297, %rd572;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r660}, %fd297;
	}
	setp.lt.s32 	%p219, %r660, 0;
	selp.b64 	%rd1125, %rd571, %rd572, %p219;
	shl.b64 	%rd1122, %rd1125, 1;
	add.s32 	%r138, %r1182, -16;
	setp.gt.s32 	%p220, %r1182, 15;
	mov.u32 	%r1182, %r138;
	@%p220 bra 	$L__BB0_107;

$L__BB0_108:
	and.b64  	%rd89, %rd1125, 9223372036854775807;
	setp.eq.s64 	%p221, %rd89, 0;
	mov.f64 	%fd534, 0d0000000000000000;
	@%p221 bra 	$L__BB0_110;

	mov.b64 	%fd299, %rd89;
	mul.f64 	%fd300, %fd299, 0d4350000000000000;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r661}, %fd300;
	}
	shr.u32 	%r662, %r661, 20;
	mov.u32 	%r663, 55;
	sub.s32 	%r664, %r663, %r662;
	sub.s32 	%r665, %r1176, %r664;
	shl.b64 	%rd573, %rd89, %r664;
	setp.lt.s32 	%p222, %r665, 1;
	mov.u32 	%r666, 1;
	sub.s32 	%r667, %r666, %r665;
	shr.u64 	%rd574, %rd573, %r667;
	add.s32 	%r668, %r665, -1;
	cvt.u64.u32 	%rd575, %r668;
	shl.b64 	%rd576, %rd575, 52;
	add.s64 	%rd577, %rd576, %rd573;
	selp.b64 	%rd578, %rd574, %rd577, %p222;
	mov.b64 	%fd534, %rd578;

$L__BB0_110:
	and.b32  	%r669, %r115, -2147483648;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r670}, %fd534;
	}
	or.b32  	%r671, %r670, %r669;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r672, %temp}, %fd534;
	}
	mov.b64 	%fd535, {%r672, %r671};
	bra.uni 	$L__BB0_114;

$L__BB0_112:
	mov.f64 	%fd301, 0d3FF0000000000000;
	add.rn.f64 	%fd535, %fd52, %fd301;

$L__BB0_114:
	add.s32 	%r1173, %r1174, -1;
	setp.gt.s32 	%p227, %r1174, 0;
	@%p227 bra 	$L__BB0_94;

	fma.rn.f64 	%fd63, %fd535, 0d3FFB96C889463A4C, 0d4001135C51C7F716;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r140}, %fd63;
	}
	and.b32  	%r673, %r140, 2147483647;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r674, %temp}, %fd63;
	}
	mov.b64 	%fd536, {%r674, %r673};
	setp.gt.u32 	%p229, %r673, 2146435071;
	or.pred  	%p230, %p229, %p187;
	@%p230 bra 	$L__BB0_132;
	bra.uni 	$L__BB0_116;

$L__BB0_132:
	setp.le.f64 	%p265, %fd537, 0d7FF0000000000000;
	setp.le.f64 	%p266, %fd536, 0d7FF0000000000000;
	and.pred  	%p267, %p266, %p265;
	@%p267 bra 	$L__BB0_134;
	bra.uni 	$L__BB0_133;

$L__BB0_134:
	setp.eq.f64 	%p268, %fd536, 0d7FF0000000000000;
	selp.f64 	%fd539, 0dFFF8000000000000, %fd63, %p268;
	bra.uni 	$L__BB0_135;

$L__BB0_116:
	setp.eq.f64 	%p231, %fd537, 0d0000000000000000;
	mov.f64 	%fd539, 0dFFF8000000000000;
	@%p231 bra 	$L__BB0_135;

	setp.ltu.f64 	%p232, %fd536, %fd537;
	mov.f64 	%fd539, %fd63;
	@%p232 bra 	$L__BB0_135;

	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r675}, %fd536;
	}
	shr.u32 	%r1184, %r675, 20;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r676}, %fd537;
	}
	shr.u32 	%r1185, %r676, 20;
	setp.ne.s32 	%p233, %r1184, 0;
	@%p233 bra 	$L__BB0_120;

	mul.f64 	%fd536, %fd536, 0d4350000000000000;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r677}, %fd536;
	}
	shr.u32 	%r678, %r677, 20;
	add.s32 	%r1184, %r678, -54;

$L__BB0_120:
	setp.ne.s32 	%p234, %r1185, 0;
	@%p234 bra 	$L__BB0_122;

	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r679}, %fd50;
	}
	shr.u32 	%r680, %r679, 20;
	add.s32 	%r1185, %r680, -54;
	mov.f64 	%fd537, %fd50;

$L__BB0_122:
	mov.b64 	%rd580, %fd536;
	and.b64  	%rd581, %rd580, 4503599627370495;
	or.b64  	%rd1130, %rd581, 4503599627370496;
	mov.b64 	%rd582, %fd537;
	and.b64  	%rd583, %rd582, 4503599627370495;
	or.b64  	%rd91, %rd583, 4503599627370496;
	sub.s32 	%r1191, %r1184, %r1185;
	not.b32 	%r681, %r1184;
	add.s32 	%r682, %r1185, %r681;
	max.s32 	%r683, %r682, -1;
	add.s32 	%r148, %r683, %r1184;
	mov.u32 	%r684, 2;
	sub.s32 	%r685, %r684, %r1185;
	add.s32 	%r686, %r685, %r148;
	and.b32  	%r1187, %r686, 3;
	setp.eq.s32 	%p235, %r1187, 0;
	@%p235 bra 	$L__BB0_124;

$L__BB0_123:
	.pragma "nounroll";
	sub.s64 	%rd584, %rd1130, %rd91;
	mov.b64 	%fd303, %rd584;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r687}, %fd303;
	}
	setp.lt.s32 	%p236, %r687, 0;
	selp.b64 	%rd1133, %rd1130, %rd584, %p236;
	shl.b64 	%rd1130, %rd1133, 1;
	add.s32 	%r1191, %r1191, -1;
	add.s32 	%r1187, %r1187, -1;
	setp.ne.s32 	%p237, %r1187, 0;
	@%p237 bra 	$L__BB0_123;

$L__BB0_124:
	mov.u32 	%r688, 1;
	sub.s32 	%r689, %r688, %r1185;
	add.s32 	%r690, %r689, %r148;
	setp.lt.u32 	%p238, %r690, 3;
	@%p238 bra 	$L__BB0_129;

	not.b32 	%r691, %r1191;
	max.s32 	%r692, %r691, -4;
	add.s32 	%r693, %r1191, %r692;
	add.s32 	%r155, %r693, 4;
	shr.u32 	%r694, %r155, 2;
	add.s32 	%r695, %r694, 1;
	and.b32  	%r1190, %r695, 3;
	setp.eq.s32 	%p239, %r1190, 0;
	@%p239 bra 	$L__BB0_127;

$L__BB0_126:
	.pragma "nounroll";
	sub.s64 	%rd586, %rd1130, %rd91;
	mov.b64 	%fd304, %rd586;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r696}, %fd304;
	}
	setp.lt.s32 	%p240, %r696, 0;
	selp.b64 	%rd587, %rd1130, %rd586, %p240;
	shl.b64 	%rd588, %rd587, 1;
	sub.s64 	%rd589, %rd588, %rd91;
	mov.b64 	%fd305, %rd589;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r697}, %fd305;
	}
	setp.lt.s32 	%p241, %r697, 0;
	selp.b64 	%rd590, %rd588, %rd589, %p241;
	shl.b64 	%rd591, %rd590, 1;
	sub.s64 	%rd592, %rd591, %rd91;
	mov.b64 	%fd306, %rd592;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r698}, %fd306;
	}
	setp.lt.s32 	%p242, %r698, 0;
	selp.b64 	%rd593, %rd591, %rd592, %p242;
	shl.b64 	%rd594, %rd593, 1;
	sub.s64 	%rd595, %rd594, %rd91;
	mov.b64 	%fd307, %rd595;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r699}, %fd307;
	}
	setp.lt.s32 	%p243, %r699, 0;
	selp.b64 	%rd1133, %rd594, %rd595, %p243;
	shl.b64 	%rd1130, %rd1133, 1;
	add.s32 	%r1191, %r1191, -4;
	add.s32 	%r1190, %r1190, -1;
	setp.ne.s32 	%p244, %r1190, 0;
	@%p244 bra 	$L__BB0_126;

$L__BB0_127:
	setp.lt.u32 	%p245, %r155, 12;
	@%p245 bra 	$L__BB0_129;

$L__BB0_128:
	sub.s64 	%rd596, %rd1130, %rd91;
	mov.b64 	%fd308, %rd596;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r700}, %fd308;
	}
	setp.lt.s32 	%p246, %r700, 0;
	selp.b64 	%rd597, %rd1130, %rd596, %p246;
	shl.b64 	%rd598, %rd597, 1;
	sub.s64 	%rd599, %rd598, %rd91;
	mov.b64 	%fd309, %rd599;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r701}, %fd309;
	}
	setp.lt.s32 	%p247, %r701, 0;
	selp.b64 	%rd600, %rd598, %rd599, %p247;
	shl.b64 	%rd601, %rd600, 1;
	sub.s64 	%rd602, %rd601, %rd91;
	mov.b64 	%fd310, %rd602;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r702}, %fd310;
	}
	setp.lt.s32 	%p248, %r702, 0;
	selp.b64 	%rd603, %rd601, %rd602, %p248;
	shl.b64 	%rd604, %rd603, 1;
	sub.s64 	%rd605, %rd604, %rd91;
	mov.b64 	%fd311, %rd605;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r703}, %fd311;
	}
	setp.lt.s32 	%p249, %r703, 0;
	selp.b64 	%rd606, %rd604, %rd605, %p249;
	shl.b64 	%rd607, %rd606, 1;
	sub.s64 	%rd608, %rd607, %rd91;
	mov.b64 	%fd312, %rd608;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r704}, %fd312;
	}
	setp.lt.s32 	%p250, %r704, 0;
	selp.b64 	%rd609, %rd607, %rd608, %p250;
	shl.b64 	%rd610, %rd609, 1;
	sub.s64 	%rd611, %rd610, %rd91;
	mov.b64 	%fd313, %rd611;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r705}, %fd313;
	}
	setp.lt.s32 	%p251, %r705, 0;
	selp.b64 	%rd612, %rd610, %rd611, %p251;
	shl.b64 	%rd613, %rd612, 1;
	sub.s64 	%rd614, %rd613, %rd91;
	mov.b64 	%fd314, %rd614;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r706}, %fd314;
	}
	setp.lt.s32 	%p252, %r706, 0;
	selp.b64 	%rd615, %rd613, %rd614, %p252;
	shl.b64 	%rd616, %rd615, 1;
	sub.s64 	%rd617, %rd616, %rd91;
	mov.b64 	%fd315, %rd617;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r707}, %fd315;
	}
	setp.lt.s32 	%p253, %r707, 0;
	selp.b64 	%rd618, %rd616, %rd617, %p253;
	shl.b64 	%rd619, %rd618, 1;
	sub.s64 	%rd620, %rd619, %rd91;
	mov.b64 	%fd316, %rd620;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r708}, %fd316;
	}
	setp.lt.s32 	%p254, %r708, 0;
	selp.b64 	%rd621, %rd619, %rd620, %p254;
	shl.b64 	%rd622, %rd621, 1;
	sub.s64 	%rd623, %rd622, %rd91;
	mov.b64 	%fd317, %rd623;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r709}, %fd317;
	}
	setp.lt.s32 	%p255, %r709, 0;
	selp.b64 	%rd624, %rd622, %rd623, %p255;
	shl.b64 	%rd625, %rd624, 1;
	sub.s64 	%rd626, %rd625, %rd91;
	mov.b64 	%fd318, %rd626;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r710}, %fd318;
	}
	setp.lt.s32 	%p256, %r710, 0;
	selp.b64 	%rd627, %rd625, %rd626, %p256;
	shl.b64 	%rd628, %rd627, 1;
	sub.s64 	%rd629, %rd628, %rd91;
	mov.b64 	%fd319, %rd629;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r711}, %fd319;
	}
	setp.lt.s32 	%p257, %r711, 0;
	selp.b64 	%rd630, %rd628, %rd629, %p257;
	shl.b64 	%rd631, %rd630, 1;
	sub.s64 	%rd632, %rd631, %rd91;
	mov.b64 	%fd320, %rd632;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r712}, %fd320;
	}
	setp.lt.s32 	%p258, %r712, 0;
	selp.b64 	%rd633, %rd631, %rd632, %p258;
	shl.b64 	%rd634, %rd633, 1;
	sub.s64 	%rd635, %rd634, %rd91;
	mov.b64 	%fd321, %rd635;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r713}, %fd321;
	}
	setp.lt.s32 	%p259, %r713, 0;
	selp.b64 	%rd636, %rd634, %rd635, %p259;
	shl.b64 	%rd637, %rd636, 1;
	sub.s64 	%rd638, %rd637, %rd91;
	mov.b64 	%fd322, %rd638;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r714}, %fd322;
	}
	setp.lt.s32 	%p260, %r714, 0;
	selp.b64 	%rd639, %rd637, %rd638, %p260;
	shl.b64 	%rd640, %rd639, 1;
	sub.s64 	%rd641, %rd640, %rd91;
	mov.b64 	%fd323, %rd641;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r715}, %fd323;
	}
	setp.lt.s32 	%p261, %r715, 0;
	selp.b64 	%rd1133, %rd640, %rd641, %p261;
	shl.b64 	%rd1130, %rd1133, 1;
	add.s32 	%r163, %r1191, -16;
	setp.gt.s32 	%p262, %r1191, 15;
	mov.u32 	%r1191, %r163;
	@%p262 bra 	$L__BB0_128;

$L__BB0_129:
	and.b64  	%rd106, %rd1133, 9223372036854775807;
	setp.eq.s64 	%p263, %rd106, 0;
	mov.f64 	%fd538, 0d0000000000000000;
	@%p263 bra 	$L__BB0_131;

	mov.b64 	%fd325, %rd106;
	mul.f64 	%fd326, %fd325, 0d4350000000000000;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r716}, %fd326;
	}
	shr.u32 	%r717, %r716, 20;
	mov.u32 	%r718, 55;
	sub.s32 	%r719, %r718, %r717;
	sub.s32 	%r720, %r1185, %r719;
	shl.b64 	%rd642, %rd106, %r719;
	setp.lt.s32 	%p264, %r720, 1;
	mov.u32 	%r721, 1;
	sub.s32 	%r722, %r721, %r720;
	shr.u64 	%rd643, %rd642, %r722;
	add.s32 	%r723, %r720, -1;
	cvt.u64.u32 	%rd644, %r723;
	shl.b64 	%rd645, %rd644, 52;
	add.s64 	%rd646, %rd645, %rd642;
	selp.b64 	%rd647, %rd643, %rd646, %p264;
	mov.b64 	%fd538, %rd647;

$L__BB0_131:
	and.b32  	%r724, %r140, -2147483648;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r725}, %fd538;
	}
	or.b32  	%r726, %r725, %r724;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r727, %temp}, %fd538;
	}
	mov.b64 	%fd539, {%r727, %r726};
	bra.uni 	$L__BB0_135;

$L__BB0_133:
	mov.f64 	%fd327, 0d3FF0000000000000;
	add.rn.f64 	%fd539, %fd63, %fd327;

$L__BB0_135:
	ld.param.u32 	%r1126, [_Z17find_seeds_kernelmj12FilterParamsPmPVi_param_2+8];
	abs.f64 	%fd328, %fd539;
	mul.f64 	%fd329, %fd328, 0d41EFFFFFFFE00000;
	cvt.rzi.u32.f64 	%r728, %fd329;
	and.b32  	%r729, %r728, 31;
	setp.ne.s32 	%p269, %r729, %r1126;
	@%p269 bra 	$L__BB0_269;

$L__BB0_136:
	ld.param.u32 	%r1127, [_Z17find_seeds_kernelmj12FilterParamsPmPVi_param_2+12];
	setp.eq.s32 	%p270, %r1127, -1;
	@%p270 bra 	$L__BB0_180;

	shr.u64 	%rd648, %rd3, 56;
	cvt.u32.u64 	%r732, %rd3;
	shr.u64 	%rd649, %rd3, 48;
	cvt.u32.u64 	%r733, %rd649;
	shr.u64 	%rd650, %rd3, 40;
	shr.u64 	%rd651, %rd3, 32;
	cvt.u32.u64 	%r734, %rd651;
	shr.u64 	%rd652, %rd3, 24;
	shr.u32 	%r735, %r732, 16;
	mov.u32 	%r1194, 16;
	and.b32  	%r736, %r732, 255;
	cvt.u16.u64 	%rs20, %rd3;
	shr.u16 	%rs21, %rs20, 8;
	cvt.u32.u16 	%r737, %rs21;
	prmt.b32 	%r738, %r737, %r736, 30212;
	cvt.u32.u64 	%r739, %rd652;
	and.b32  	%r740, %r735, 255;
	prmt.b32 	%r741, %r739, %r740, 30212;
	cvt.u32.u64 	%r742, %rd650;
	and.b32  	%r743, %r734, 255;
	prmt.b32 	%r744, %r742, %r743, 30212;
	cvt.u32.u64 	%r745, %rd648;
	and.b32  	%r746, %r733, 255;
	prmt.b32 	%r747, %r745, %r746, 30212;
	mov.u16 	%rs22, 12339;
	mov.u16 	%rs23, 13885;
	mov.u16 	%rs24, 16180;
	mov.u16 	%rs25, 13374;
	prmt.b32 	%r748, %r747, %r744, 4180;
	prmt.b32 	%r749, %r741, %r738, 4180;
	mov.b32 	%r750, {%rs25, %rs24};
	mov.b32 	%r751, {%rs23, %rs22};
	st.local.v4.u32 	[%rd2], {%r751, %r750, %r749, %r748};
	mov.f64 	%fd544, 0d3FF0000000000000;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r752}, %fd544;
	}
	and.b32  	%r165, %r752, 2147483647;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r753, %temp}, %fd544;
	}
	mov.b64 	%fd546, {%r753, %r165};
	mul.f64 	%fd75, %fd546, 0d4350000000000000;
	mov.u32 	%r1193, 15;

$L__BB0_138:
	mov.u32 	%r167, %r1194;
	mov.u32 	%r1194, %r1193;
	cvt.s64.s32 	%rd653, %r1194;
	add.s64 	%rd654, %rd2, %rd653;
	ld.local.s8 	%rs26, [%rd654];
	cvt.rn.f64.s16 	%fd331, %rs26;
	mov.f64 	%fd332, 0d3FF1FB9C7406BB70;
	div.rn.f64 	%fd333, %fd332, %fd544;
	mul.f64 	%fd334, %fd333, %fd331;
	mul.f64 	%fd335, %fd334, 0d400921FB54442EEA;
	cvt.rn.f64.s32 	%fd336, %r167;
	fma.rn.f64 	%fd77, %fd336, 0d400921FB54442EEA, %fd335;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r168}, %fd77;
	}
	and.b32  	%r754, %r168, 2147483647;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r755, %temp}, %fd77;
	}
	mov.b64 	%fd541, {%r755, %r754};
	setp.gt.u32 	%p271, %r754, 2146435071;
	setp.gt.u32 	%p272, %r165, 2146435071;
	or.pred  	%p273, %p271, %p272;
	@%p273 bra 	$L__BB0_155;
	bra.uni 	$L__BB0_139;

$L__BB0_155:
	setp.le.f64 	%p308, %fd546, 0d7FF0000000000000;
	setp.le.f64 	%p309, %fd541, 0d7FF0000000000000;
	and.pred  	%p310, %p309, %p308;
	@%p310 bra 	$L__BB0_157;
	bra.uni 	$L__BB0_156;

$L__BB0_157:
	setp.eq.f64 	%p311, %fd541, 0d7FF0000000000000;
	selp.f64 	%fd544, 0dFFF8000000000000, %fd77, %p311;
	bra.uni 	$L__BB0_158;

$L__BB0_139:
	setp.eq.f64 	%p274, %fd546, 0d0000000000000000;
	mov.f64 	%fd544, 0dFFF8000000000000;
	@%p274 bra 	$L__BB0_158;

	setp.ltu.f64 	%p275, %fd541, %fd546;
	mov.f64 	%fd544, %fd77;
	@%p275 bra 	$L__BB0_158;

	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r756}, %fd541;
	}
	shr.u32 	%r1195, %r756, 20;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r757}, %fd546;
	}
	shr.u32 	%r1196, %r757, 20;
	setp.ne.s32 	%p276, %r1195, 0;
	@%p276 bra 	$L__BB0_143;

	mul.f64 	%fd541, %fd541, 0d4350000000000000;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r758}, %fd541;
	}
	shr.u32 	%r759, %r758, 20;
	add.s32 	%r1195, %r759, -54;

$L__BB0_143:
	setp.ne.s32 	%p277, %r1196, 0;
	mov.f64 	%fd542, %fd546;
	@%p277 bra 	$L__BB0_145;

	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r760}, %fd75;
	}
	shr.u32 	%r761, %r760, 20;
	add.s32 	%r1196, %r761, -54;
	mov.f64 	%fd542, %fd75;

$L__BB0_145:
	mov.b64 	%rd656, %fd541;
	and.b64  	%rd657, %rd656, 4503599627370495;
	or.b64  	%rd1138, %rd657, 4503599627370496;
	mov.b64 	%rd658, %fd542;
	and.b64  	%rd659, %rd658, 4503599627370495;
	or.b64  	%rd108, %rd659, 4503599627370496;
	sub.s32 	%r1202, %r1195, %r1196;
	not.b32 	%r762, %r1195;
	add.s32 	%r763, %r1196, %r762;
	max.s32 	%r764, %r763, -1;
	add.s32 	%r176, %r764, %r1195;
	mov.u32 	%r765, 2;
	sub.s32 	%r766, %r765, %r1196;
	add.s32 	%r767, %r766, %r176;
	and.b32  	%r1198, %r767, 3;
	setp.eq.s32 	%p278, %r1198, 0;
	@%p278 bra 	$L__BB0_147;

$L__BB0_146:
	.pragma "nounroll";
	sub.s64 	%rd660, %rd1138, %rd108;
	mov.b64 	%fd338, %rd660;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r768}, %fd338;
	}
	setp.lt.s32 	%p279, %r768, 0;
	selp.b64 	%rd1141, %rd1138, %rd660, %p279;
	shl.b64 	%rd1138, %rd1141, 1;
	add.s32 	%r1202, %r1202, -1;
	add.s32 	%r1198, %r1198, -1;
	setp.ne.s32 	%p280, %r1198, 0;
	@%p280 bra 	$L__BB0_146;

$L__BB0_147:
	mov.u32 	%r769, 1;
	sub.s32 	%r770, %r769, %r1196;
	add.s32 	%r771, %r770, %r176;
	setp.lt.u32 	%p281, %r771, 3;
	@%p281 bra 	$L__BB0_152;

	not.b32 	%r772, %r1202;
	max.s32 	%r773, %r772, -4;
	add.s32 	%r774, %r1202, %r773;
	add.s32 	%r183, %r774, 4;
	shr.u32 	%r775, %r183, 2;
	add.s32 	%r776, %r775, 1;
	and.b32  	%r1201, %r776, 3;
	setp.eq.s32 	%p282, %r1201, 0;
	@%p282 bra 	$L__BB0_150;

$L__BB0_149:
	.pragma "nounroll";
	sub.s64 	%rd662, %rd1138, %rd108;
	mov.b64 	%fd339, %rd662;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r777}, %fd339;
	}
	setp.lt.s32 	%p283, %r777, 0;
	selp.b64 	%rd663, %rd1138, %rd662, %p283;
	shl.b64 	%rd664, %rd663, 1;
	sub.s64 	%rd665, %rd664, %rd108;
	mov.b64 	%fd340, %rd665;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r778}, %fd340;
	}
	setp.lt.s32 	%p284, %r778, 0;
	selp.b64 	%rd666, %rd664, %rd665, %p284;
	shl.b64 	%rd667, %rd666, 1;
	sub.s64 	%rd668, %rd667, %rd108;
	mov.b64 	%fd341, %rd668;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r779}, %fd341;
	}
	setp.lt.s32 	%p285, %r779, 0;
	selp.b64 	%rd669, %rd667, %rd668, %p285;
	shl.b64 	%rd670, %rd669, 1;
	sub.s64 	%rd671, %rd670, %rd108;
	mov.b64 	%fd342, %rd671;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r780}, %fd342;
	}
	setp.lt.s32 	%p286, %r780, 0;
	selp.b64 	%rd1141, %rd670, %rd671, %p286;
	shl.b64 	%rd1138, %rd1141, 1;
	add.s32 	%r1202, %r1202, -4;
	add.s32 	%r1201, %r1201, -1;
	setp.ne.s32 	%p287, %r1201, 0;
	@%p287 bra 	$L__BB0_149;

$L__BB0_150:
	setp.lt.u32 	%p288, %r183, 12;
	@%p288 bra 	$L__BB0_152;

$L__BB0_151:
	sub.s64 	%rd672, %rd1138, %rd108;
	mov.b64 	%fd343, %rd672;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r781}, %fd343;
	}
	setp.lt.s32 	%p289, %r781, 0;
	selp.b64 	%rd673, %rd1138, %rd672, %p289;
	shl.b64 	%rd674, %rd673, 1;
	sub.s64 	%rd675, %rd674, %rd108;
	mov.b64 	%fd344, %rd675;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r782}, %fd344;
	}
	setp.lt.s32 	%p290, %r782, 0;
	selp.b64 	%rd676, %rd674, %rd675, %p290;
	shl.b64 	%rd677, %rd676, 1;
	sub.s64 	%rd678, %rd677, %rd108;
	mov.b64 	%fd345, %rd678;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r783}, %fd345;
	}
	setp.lt.s32 	%p291, %r783, 0;
	selp.b64 	%rd679, %rd677, %rd678, %p291;
	shl.b64 	%rd680, %rd679, 1;
	sub.s64 	%rd681, %rd680, %rd108;
	mov.b64 	%fd346, %rd681;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r784}, %fd346;
	}
	setp.lt.s32 	%p292, %r784, 0;
	selp.b64 	%rd682, %rd680, %rd681, %p292;
	shl.b64 	%rd683, %rd682, 1;
	sub.s64 	%rd684, %rd683, %rd108;
	mov.b64 	%fd347, %rd684;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r785}, %fd347;
	}
	setp.lt.s32 	%p293, %r785, 0;
	selp.b64 	%rd685, %rd683, %rd684, %p293;
	shl.b64 	%rd686, %rd685, 1;
	sub.s64 	%rd687, %rd686, %rd108;
	mov.b64 	%fd348, %rd687;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r786}, %fd348;
	}
	setp.lt.s32 	%p294, %r786, 0;
	selp.b64 	%rd688, %rd686, %rd687, %p294;
	shl.b64 	%rd689, %rd688, 1;
	sub.s64 	%rd690, %rd689, %rd108;
	mov.b64 	%fd349, %rd690;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r787}, %fd349;
	}
	setp.lt.s32 	%p295, %r787, 0;
	selp.b64 	%rd691, %rd689, %rd690, %p295;
	shl.b64 	%rd692, %rd691, 1;
	sub.s64 	%rd693, %rd692, %rd108;
	mov.b64 	%fd350, %rd693;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r788}, %fd350;
	}
	setp.lt.s32 	%p296, %r788, 0;
	selp.b64 	%rd694, %rd692, %rd693, %p296;
	shl.b64 	%rd695, %rd694, 1;
	sub.s64 	%rd696, %rd695, %rd108;
	mov.b64 	%fd351, %rd696;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r789}, %fd351;
	}
	setp.lt.s32 	%p297, %r789, 0;
	selp.b64 	%rd697, %rd695, %rd696, %p297;
	shl.b64 	%rd698, %rd697, 1;
	sub.s64 	%rd699, %rd698, %rd108;
	mov.b64 	%fd352, %rd699;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r790}, %fd352;
	}
	setp.lt.s32 	%p298, %r790, 0;
	selp.b64 	%rd700, %rd698, %rd699, %p298;
	shl.b64 	%rd701, %rd700, 1;
	sub.s64 	%rd702, %rd701, %rd108;
	mov.b64 	%fd353, %rd702;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r791}, %fd353;
	}
	setp.lt.s32 	%p299, %r791, 0;
	selp.b64 	%rd703, %rd701, %rd702, %p299;
	shl.b64 	%rd704, %rd703, 1;
	sub.s64 	%rd705, %rd704, %rd108;
	mov.b64 	%fd354, %rd705;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r792}, %fd354;
	}
	setp.lt.s32 	%p300, %r792, 0;
	selp.b64 	%rd706, %rd704, %rd705, %p300;
	shl.b64 	%rd707, %rd706, 1;
	sub.s64 	%rd708, %rd707, %rd108;
	mov.b64 	%fd355, %rd708;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r793}, %fd355;
	}
	setp.lt.s32 	%p301, %r793, 0;
	selp.b64 	%rd709, %rd707, %rd708, %p301;
	shl.b64 	%rd710, %rd709, 1;
	sub.s64 	%rd711, %rd710, %rd108;
	mov.b64 	%fd356, %rd711;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r794}, %fd356;
	}
	setp.lt.s32 	%p302, %r794, 0;
	selp.b64 	%rd712, %rd710, %rd711, %p302;
	shl.b64 	%rd713, %rd712, 1;
	sub.s64 	%rd714, %rd713, %rd108;
	mov.b64 	%fd357, %rd714;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r795}, %fd357;
	}
	setp.lt.s32 	%p303, %r795, 0;
	selp.b64 	%rd715, %rd713, %rd714, %p303;
	shl.b64 	%rd716, %rd715, 1;
	sub.s64 	%rd717, %rd716, %rd108;
	mov.b64 	%fd358, %rd717;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r796}, %fd358;
	}
	setp.lt.s32 	%p304, %r796, 0;
	selp.b64 	%rd1141, %rd716, %rd717, %p304;
	shl.b64 	%rd1138, %rd1141, 1;
	add.s32 	%r191, %r1202, -16;
	setp.gt.s32 	%p305, %r1202, 15;
	mov.u32 	%r1202, %r191;
	@%p305 bra 	$L__BB0_151;

$L__BB0_152:
	and.b64  	%rd123, %rd1141, 9223372036854775807;
	setp.eq.s64 	%p306, %rd123, 0;
	mov.f64 	%fd543, 0d0000000000000000;
	@%p306 bra 	$L__BB0_154;

	mov.b64 	%fd360, %rd123;
	mul.f64 	%fd361, %fd360, 0d4350000000000000;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r797}, %fd361;
	}
	shr.u32 	%r798, %r797, 20;
	mov.u32 	%r799, 55;
	sub.s32 	%r800, %r799, %r798;
	sub.s32 	%r801, %r1196, %r800;
	shl.b64 	%rd718, %rd123, %r800;
	setp.lt.s32 	%p307, %r801, 1;
	mov.u32 	%r802, 1;
	sub.s32 	%r803, %r802, %r801;
	shr.u64 	%rd719, %rd718, %r803;
	add.s32 	%r804, %r801, -1;
	cvt.u64.u32 	%rd720, %r804;
	shl.b64 	%rd721, %rd720, 52;
	add.s64 	%rd722, %rd721, %rd718;
	selp.b64 	%rd723, %rd719, %rd722, %p307;
	mov.b64 	%fd543, %rd723;

$L__BB0_154:
	and.b32  	%r805, %r168, -2147483648;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r806}, %fd543;
	}
	or.b32  	%r807, %r806, %r805;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r808, %temp}, %fd543;
	}
	mov.b64 	%fd544, {%r808, %r807};
	bra.uni 	$L__BB0_158;

$L__BB0_156:
	mov.f64 	%fd362, 0d3FF0000000000000;
	add.rn.f64 	%fd544, %fd77, %fd362;

$L__BB0_158:
	add.s32 	%r1193, %r1194, -1;
	setp.gt.s32 	%p312, %r1194, 0;
	@%p312 bra 	$L__BB0_138;

	fma.rn.f64 	%fd88, %fd544, 0d3FFB96C889463A4C, 0d4001135C51C7F716;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r193}, %fd88;
	}
	and.b32  	%r809, %r193, 2147483647;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r810, %temp}, %fd88;
	}
	mov.b64 	%fd545, {%r810, %r809};
	setp.gt.u32 	%p314, %r809, 2146435071;
	or.pred  	%p315, %p314, %p272;
	@%p315 bra 	$L__BB0_176;
	bra.uni 	$L__BB0_160;

$L__BB0_176:
	setp.le.f64 	%p350, %fd546, 0d7FF0000000000000;
	setp.le.f64 	%p351, %fd545, 0d7FF0000000000000;
	and.pred  	%p352, %p351, %p350;
	@%p352 bra 	$L__BB0_178;
	bra.uni 	$L__BB0_177;

$L__BB0_178:
	setp.eq.f64 	%p353, %fd545, 0d7FF0000000000000;
	selp.f64 	%fd548, 0dFFF8000000000000, %fd88, %p353;
	bra.uni 	$L__BB0_179;

$L__BB0_160:
	setp.eq.f64 	%p316, %fd546, 0d0000000000000000;
	mov.f64 	%fd548, 0dFFF8000000000000;
	@%p316 bra 	$L__BB0_179;

	setp.ltu.f64 	%p317, %fd545, %fd546;
	mov.f64 	%fd548, %fd88;
	@%p317 bra 	$L__BB0_179;

	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r811}, %fd545;
	}
	shr.u32 	%r1204, %r811, 20;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r812}, %fd546;
	}
	shr.u32 	%r1205, %r812, 20;
	setp.ne.s32 	%p318, %r1204, 0;
	@%p318 bra 	$L__BB0_164;

	mul.f64 	%fd545, %fd545, 0d4350000000000000;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r813}, %fd545;
	}
	shr.u32 	%r814, %r813, 20;
	add.s32 	%r1204, %r814, -54;

$L__BB0_164:
	setp.ne.s32 	%p319, %r1205, 0;
	@%p319 bra 	$L__BB0_166;

	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r815}, %fd75;
	}
	shr.u32 	%r816, %r815, 20;
	add.s32 	%r1205, %r816, -54;
	mov.f64 	%fd546, %fd75;

$L__BB0_166:
	mov.b64 	%rd725, %fd545;
	and.b64  	%rd726, %rd725, 4503599627370495;
	or.b64  	%rd1146, %rd726, 4503599627370496;
	mov.b64 	%rd727, %fd546;
	and.b64  	%rd728, %rd727, 4503599627370495;
	or.b64  	%rd125, %rd728, 4503599627370496;
	sub.s32 	%r1211, %r1204, %r1205;
	not.b32 	%r817, %r1204;
	add.s32 	%r818, %r1205, %r817;
	max.s32 	%r819, %r818, -1;
	add.s32 	%r201, %r819, %r1204;
	mov.u32 	%r820, 2;
	sub.s32 	%r821, %r820, %r1205;
	add.s32 	%r822, %r821, %r201;
	and.b32  	%r1207, %r822, 3;
	setp.eq.s32 	%p320, %r1207, 0;
	@%p320 bra 	$L__BB0_168;

$L__BB0_167:
	.pragma "nounroll";
	sub.s64 	%rd729, %rd1146, %rd125;
	mov.b64 	%fd364, %rd729;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r823}, %fd364;
	}
	setp.lt.s32 	%p321, %r823, 0;
	selp.b64 	%rd1149, %rd1146, %rd729, %p321;
	shl.b64 	%rd1146, %rd1149, 1;
	add.s32 	%r1211, %r1211, -1;
	add.s32 	%r1207, %r1207, -1;
	setp.ne.s32 	%p322, %r1207, 0;
	@%p322 bra 	$L__BB0_167;

$L__BB0_168:
	mov.u32 	%r824, 1;
	sub.s32 	%r825, %r824, %r1205;
	add.s32 	%r826, %r825, %r201;
	setp.lt.u32 	%p323, %r826, 3;
	@%p323 bra 	$L__BB0_173;

	not.b32 	%r827, %r1211;
	max.s32 	%r828, %r827, -4;
	add.s32 	%r829, %r1211, %r828;
	add.s32 	%r208, %r829, 4;
	shr.u32 	%r830, %r208, 2;
	add.s32 	%r831, %r830, 1;
	and.b32  	%r1210, %r831, 3;
	setp.eq.s32 	%p324, %r1210, 0;
	@%p324 bra 	$L__BB0_171;

$L__BB0_170:
	.pragma "nounroll";
	sub.s64 	%rd731, %rd1146, %rd125;
	mov.b64 	%fd365, %rd731;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r832}, %fd365;
	}
	setp.lt.s32 	%p325, %r832, 0;
	selp.b64 	%rd732, %rd1146, %rd731, %p325;
	shl.b64 	%rd733, %rd732, 1;
	sub.s64 	%rd734, %rd733, %rd125;
	mov.b64 	%fd366, %rd734;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r833}, %fd366;
	}
	setp.lt.s32 	%p326, %r833, 0;
	selp.b64 	%rd735, %rd733, %rd734, %p326;
	shl.b64 	%rd736, %rd735, 1;
	sub.s64 	%rd737, %rd736, %rd125;
	mov.b64 	%fd367, %rd737;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r834}, %fd367;
	}
	setp.lt.s32 	%p327, %r834, 0;
	selp.b64 	%rd738, %rd736, %rd737, %p327;
	shl.b64 	%rd739, %rd738, 1;
	sub.s64 	%rd740, %rd739, %rd125;
	mov.b64 	%fd368, %rd740;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r835}, %fd368;
	}
	setp.lt.s32 	%p328, %r835, 0;
	selp.b64 	%rd1149, %rd739, %rd740, %p328;
	shl.b64 	%rd1146, %rd1149, 1;
	add.s32 	%r1211, %r1211, -4;
	add.s32 	%r1210, %r1210, -1;
	setp.ne.s32 	%p329, %r1210, 0;
	@%p329 bra 	$L__BB0_170;

$L__BB0_171:
	setp.lt.u32 	%p330, %r208, 12;
	@%p330 bra 	$L__BB0_173;

$L__BB0_172:
	sub.s64 	%rd741, %rd1146, %rd125;
	mov.b64 	%fd369, %rd741;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r836}, %fd369;
	}
	setp.lt.s32 	%p331, %r836, 0;
	selp.b64 	%rd742, %rd1146, %rd741, %p331;
	shl.b64 	%rd743, %rd742, 1;
	sub.s64 	%rd744, %rd743, %rd125;
	mov.b64 	%fd370, %rd744;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r837}, %fd370;
	}
	setp.lt.s32 	%p332, %r837, 0;
	selp.b64 	%rd745, %rd743, %rd744, %p332;
	shl.b64 	%rd746, %rd745, 1;
	sub.s64 	%rd747, %rd746, %rd125;
	mov.b64 	%fd371, %rd747;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r838}, %fd371;
	}
	setp.lt.s32 	%p333, %r838, 0;
	selp.b64 	%rd748, %rd746, %rd747, %p333;
	shl.b64 	%rd749, %rd748, 1;
	sub.s64 	%rd750, %rd749, %rd125;
	mov.b64 	%fd372, %rd750;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r839}, %fd372;
	}
	setp.lt.s32 	%p334, %r839, 0;
	selp.b64 	%rd751, %rd749, %rd750, %p334;
	shl.b64 	%rd752, %rd751, 1;
	sub.s64 	%rd753, %rd752, %rd125;
	mov.b64 	%fd373, %rd753;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r840}, %fd373;
	}
	setp.lt.s32 	%p335, %r840, 0;
	selp.b64 	%rd754, %rd752, %rd753, %p335;
	shl.b64 	%rd755, %rd754, 1;
	sub.s64 	%rd756, %rd755, %rd125;
	mov.b64 	%fd374, %rd756;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r841}, %fd374;
	}
	setp.lt.s32 	%p336, %r841, 0;
	selp.b64 	%rd757, %rd755, %rd756, %p336;
	shl.b64 	%rd758, %rd757, 1;
	sub.s64 	%rd759, %rd758, %rd125;
	mov.b64 	%fd375, %rd759;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r842}, %fd375;
	}
	setp.lt.s32 	%p337, %r842, 0;
	selp.b64 	%rd760, %rd758, %rd759, %p337;
	shl.b64 	%rd761, %rd760, 1;
	sub.s64 	%rd762, %rd761, %rd125;
	mov.b64 	%fd376, %rd762;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r843}, %fd376;
	}
	setp.lt.s32 	%p338, %r843, 0;
	selp.b64 	%rd763, %rd761, %rd762, %p338;
	shl.b64 	%rd764, %rd763, 1;
	sub.s64 	%rd765, %rd764, %rd125;
	mov.b64 	%fd377, %rd765;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r844}, %fd377;
	}
	setp.lt.s32 	%p339, %r844, 0;
	selp.b64 	%rd766, %rd764, %rd765, %p339;
	shl.b64 	%rd767, %rd766, 1;
	sub.s64 	%rd768, %rd767, %rd125;
	mov.b64 	%fd378, %rd768;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r845}, %fd378;
	}
	setp.lt.s32 	%p340, %r845, 0;
	selp.b64 	%rd769, %rd767, %rd768, %p340;
	shl.b64 	%rd770, %rd769, 1;
	sub.s64 	%rd771, %rd770, %rd125;
	mov.b64 	%fd379, %rd771;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r846}, %fd379;
	}
	setp.lt.s32 	%p341, %r846, 0;
	selp.b64 	%rd772, %rd770, %rd771, %p341;
	shl.b64 	%rd773, %rd772, 1;
	sub.s64 	%rd774, %rd773, %rd125;
	mov.b64 	%fd380, %rd774;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r847}, %fd380;
	}
	setp.lt.s32 	%p342, %r847, 0;
	selp.b64 	%rd775, %rd773, %rd774, %p342;
	shl.b64 	%rd776, %rd775, 1;
	sub.s64 	%rd777, %rd776, %rd125;
	mov.b64 	%fd381, %rd777;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r848}, %fd381;
	}
	setp.lt.s32 	%p343, %r848, 0;
	selp.b64 	%rd778, %rd776, %rd777, %p343;
	shl.b64 	%rd779, %rd778, 1;
	sub.s64 	%rd780, %rd779, %rd125;
	mov.b64 	%fd382, %rd780;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r849}, %fd382;
	}
	setp.lt.s32 	%p344, %r849, 0;
	selp.b64 	%rd781, %rd779, %rd780, %p344;
	shl.b64 	%rd782, %rd781, 1;
	sub.s64 	%rd783, %rd782, %rd125;
	mov.b64 	%fd383, %rd783;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r850}, %fd383;
	}
	setp.lt.s32 	%p345, %r850, 0;
	selp.b64 	%rd784, %rd782, %rd783, %p345;
	shl.b64 	%rd785, %rd784, 1;
	sub.s64 	%rd786, %rd785, %rd125;
	mov.b64 	%fd384, %rd786;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r851}, %fd384;
	}
	setp.lt.s32 	%p346, %r851, 0;
	selp.b64 	%rd1149, %rd785, %rd786, %p346;
	shl.b64 	%rd1146, %rd1149, 1;
	add.s32 	%r216, %r1211, -16;
	setp.gt.s32 	%p347, %r1211, 15;
	mov.u32 	%r1211, %r216;
	@%p347 bra 	$L__BB0_172;

$L__BB0_173:
	and.b64  	%rd140, %rd1149, 9223372036854775807;
	setp.eq.s64 	%p348, %rd140, 0;
	mov.f64 	%fd547, 0d0000000000000000;
	@%p348 bra 	$L__BB0_175;

	mov.b64 	%fd386, %rd140;
	mul.f64 	%fd387, %fd386, 0d4350000000000000;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r852}, %fd387;
	}
	shr.u32 	%r853, %r852, 20;
	mov.u32 	%r854, 55;
	sub.s32 	%r855, %r854, %r853;
	sub.s32 	%r856, %r1205, %r855;
	shl.b64 	%rd787, %rd140, %r855;
	setp.lt.s32 	%p349, %r856, 1;
	mov.u32 	%r857, 1;
	sub.s32 	%r858, %r857, %r856;
	shr.u64 	%rd788, %rd787, %r858;
	add.s32 	%r859, %r856, -1;
	cvt.u64.u32 	%rd789, %r859;
	shl.b64 	%rd790, %rd789, 52;
	add.s64 	%rd791, %rd790, %rd787;
	selp.b64 	%rd792, %rd788, %rd791, %p349;
	mov.b64 	%fd547, %rd792;

$L__BB0_175:
	and.b32  	%r860, %r193, -2147483648;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r861}, %fd547;
	}
	or.b32  	%r862, %r861, %r860;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r863, %temp}, %fd547;
	}
	mov.b64 	%fd548, {%r863, %r862};
	bra.uni 	$L__BB0_179;

$L__BB0_177:
	mov.f64 	%fd388, 0d3FF0000000000000;
	add.rn.f64 	%fd548, %fd88, %fd388;

$L__BB0_179:
	ld.param.u32 	%r1128, [_Z17find_seeds_kernelmj12FilterParamsPmPVi_param_2+12];
	abs.f64 	%fd389, %fd548;
	mul.f64 	%fd390, %fd389, 0d41EFFFFFFFE00000;
	cvt.rzi.u32.f64 	%r864, %fd390;
	mul.wide.u32 	%rd793, %r864, -2004318071;
	shr.u64 	%rd794, %rd793, 35;
	cvt.u32.u64 	%r865, %rd794;
	mul.lo.s32 	%r866, %r865, 15;
	sub.s32 	%r867, %r864, %r866;
	setp.ne.s32 	%p354, %r867, %r1128;
	@%p354 bra 	$L__BB0_269;

$L__BB0_180:
	setp.eq.s16 	%p355, %rs6, 0;
	@%p355 bra 	$L__BB0_267;

	shr.u64 	%rd795, %rd3, 56;
	cvt.u32.u64 	%r870, %rd3;
	shr.u64 	%rd796, %rd3, 48;
	cvt.u32.u64 	%r871, %rd796;
	shr.u64 	%rd797, %rd3, 40;
	shr.u64 	%rd798, %rd3, 32;
	cvt.u32.u64 	%r872, %rd798;
	shr.u64 	%rd799, %rd3, 24;
	shr.u32 	%r873, %r870, 16;
	mov.u32 	%r1214, 16;
	and.b32  	%r874, %r870, 255;
	cvt.u16.u64 	%rs28, %rd3;
	shr.u16 	%rs29, %rs28, 8;
	cvt.u32.u16 	%r875, %rs29;
	prmt.b32 	%r876, %r875, %r874, 30212;
	cvt.u32.u64 	%r877, %rd799;
	and.b32  	%r878, %r873, 255;
	prmt.b32 	%r879, %r877, %r878, 30212;
	prmt.b32 	%r217, %r879, %r876, 4180;
	cvt.u32.u64 	%r880, %rd797;
	and.b32  	%r881, %r872, 255;
	prmt.b32 	%r882, %r880, %r881, 30212;
	cvt.u32.u64 	%r883, %rd795;
	and.b32  	%r884, %r871, 255;
	prmt.b32 	%r885, %r883, %r884, 30212;
	prmt.b32 	%r218, %r885, %r882, 4180;
	mov.u32 	%r886, 976303413;
	mov.u32 	%r887, 959525169;
	st.local.v4.u32 	[%rd2], {%r887, %r886, %r217, %r218};
	mov.f64 	%fd553, 0d3FF0000000000000;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r888}, %fd553;
	}
	and.b32  	%r219, %r888, 2147483647;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r889, %temp}, %fd553;
	}
	mov.b64 	%fd564, {%r889, %r219};
	mul.f64 	%fd100, %fd564, 0d4350000000000000;
	mov.u32 	%r1213, 15;

$L__BB0_182:
	mov.u32 	%r221, %r1214;
	mov.u32 	%r1214, %r1213;
	cvt.s64.s32 	%rd800, %r1214;
	add.s64 	%rd801, %rd2, %rd800;
	ld.local.s8 	%rs30, [%rd801];
	cvt.rn.f64.s16 	%fd392, %rs30;
	mov.f64 	%fd393, 0d3FF1FB9C7406BB70;
	div.rn.f64 	%fd394, %fd393, %fd553;
	mul.f64 	%fd395, %fd394, %fd392;
	mul.f64 	%fd396, %fd395, 0d400921FB54442EEA;
	cvt.rn.f64.s32 	%fd397, %r221;
	fma.rn.f64 	%fd102, %fd397, 0d400921FB54442EEA, %fd396;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r222}, %fd102;
	}
	and.b32  	%r890, %r222, 2147483647;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r891, %temp}, %fd102;
	}
	mov.b64 	%fd550, {%r891, %r890};
	setp.gt.u32 	%p356, %r890, 2146435071;
	setp.gt.u32 	%p357, %r219, 2146435071;
	or.pred  	%p358, %p356, %p357;
	@%p358 bra 	$L__BB0_199;
	bra.uni 	$L__BB0_183;

$L__BB0_199:
	setp.le.f64 	%p393, %fd564, 0d7FF0000000000000;
	setp.le.f64 	%p394, %fd550, 0d7FF0000000000000;
	and.pred  	%p395, %p394, %p393;
	@%p395 bra 	$L__BB0_201;
	bra.uni 	$L__BB0_200;

$L__BB0_201:
	setp.eq.f64 	%p396, %fd550, 0d7FF0000000000000;
	selp.f64 	%fd553, 0dFFF8000000000000, %fd102, %p396;
	bra.uni 	$L__BB0_202;

$L__BB0_183:
	setp.eq.f64 	%p359, %fd564, 0d0000000000000000;
	mov.f64 	%fd553, 0dFFF8000000000000;
	@%p359 bra 	$L__BB0_202;

	setp.ltu.f64 	%p360, %fd550, %fd564;
	mov.f64 	%fd553, %fd102;
	@%p360 bra 	$L__BB0_202;

	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r892}, %fd550;
	}
	shr.u32 	%r1215, %r892, 20;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r893}, %fd564;
	}
	shr.u32 	%r1216, %r893, 20;
	setp.ne.s32 	%p361, %r1215, 0;
	@%p361 bra 	$L__BB0_187;

	mul.f64 	%fd550, %fd550, 0d4350000000000000;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r894}, %fd550;
	}
	shr.u32 	%r895, %r894, 20;
	add.s32 	%r1215, %r895, -54;

$L__BB0_187:
	setp.ne.s32 	%p362, %r1216, 0;
	mov.f64 	%fd551, %fd564;
	@%p362 bra 	$L__BB0_189;

	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r896}, %fd100;
	}
	shr.u32 	%r897, %r896, 20;
	add.s32 	%r1216, %r897, -54;
	mov.f64 	%fd551, %fd100;

$L__BB0_189:
	mov.b64 	%rd803, %fd550;
	and.b64  	%rd804, %rd803, 4503599627370495;
	or.b64  	%rd1154, %rd804, 4503599627370496;
	mov.b64 	%rd805, %fd551;
	and.b64  	%rd806, %rd805, 4503599627370495;
	or.b64  	%rd142, %rd806, 4503599627370496;
	sub.s32 	%r1222, %r1215, %r1216;
	not.b32 	%r898, %r1215;
	add.s32 	%r899, %r1216, %r898;
	max.s32 	%r900, %r899, -1;
	add.s32 	%r230, %r900, %r1215;
	mov.u32 	%r901, 2;
	sub.s32 	%r902, %r901, %r1216;
	add.s32 	%r903, %r902, %r230;
	and.b32  	%r1218, %r903, 3;
	setp.eq.s32 	%p363, %r1218, 0;
	@%p363 bra 	$L__BB0_191;

$L__BB0_190:
	.pragma "nounroll";
	sub.s64 	%rd807, %rd1154, %rd142;
	mov.b64 	%fd399, %rd807;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r904}, %fd399;
	}
	setp.lt.s32 	%p364, %r904, 0;
	selp.b64 	%rd1157, %rd1154, %rd807, %p364;
	shl.b64 	%rd1154, %rd1157, 1;
	add.s32 	%r1222, %r1222, -1;
	add.s32 	%r1218, %r1218, -1;
	setp.ne.s32 	%p365, %r1218, 0;
	@%p365 bra 	$L__BB0_190;

$L__BB0_191:
	mov.u32 	%r905, 1;
	sub.s32 	%r906, %r905, %r1216;
	add.s32 	%r907, %r906, %r230;
	setp.lt.u32 	%p366, %r907, 3;
	@%p366 bra 	$L__BB0_196;

	not.b32 	%r908, %r1222;
	max.s32 	%r909, %r908, -4;
	add.s32 	%r910, %r1222, %r909;
	add.s32 	%r237, %r910, 4;
	shr.u32 	%r911, %r237, 2;
	add.s32 	%r912, %r911, 1;
	and.b32  	%r1221, %r912, 3;
	setp.eq.s32 	%p367, %r1221, 0;
	@%p367 bra 	$L__BB0_194;

$L__BB0_193:
	.pragma "nounroll";
	sub.s64 	%rd809, %rd1154, %rd142;
	mov.b64 	%fd400, %rd809;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r913}, %fd400;
	}
	setp.lt.s32 	%p368, %r913, 0;
	selp.b64 	%rd810, %rd1154, %rd809, %p368;
	shl.b64 	%rd811, %rd810, 1;
	sub.s64 	%rd812, %rd811, %rd142;
	mov.b64 	%fd401, %rd812;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r914}, %fd401;
	}
	setp.lt.s32 	%p369, %r914, 0;
	selp.b64 	%rd813, %rd811, %rd812, %p369;
	shl.b64 	%rd814, %rd813, 1;
	sub.s64 	%rd815, %rd814, %rd142;
	mov.b64 	%fd402, %rd815;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r915}, %fd402;
	}
	setp.lt.s32 	%p370, %r915, 0;
	selp.b64 	%rd816, %rd814, %rd815, %p370;
	shl.b64 	%rd817, %rd816, 1;
	sub.s64 	%rd818, %rd817, %rd142;
	mov.b64 	%fd403, %rd818;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r916}, %fd403;
	}
	setp.lt.s32 	%p371, %r916, 0;
	selp.b64 	%rd1157, %rd817, %rd818, %p371;
	shl.b64 	%rd1154, %rd1157, 1;
	add.s32 	%r1222, %r1222, -4;
	add.s32 	%r1221, %r1221, -1;
	setp.ne.s32 	%p372, %r1221, 0;
	@%p372 bra 	$L__BB0_193;

$L__BB0_194:
	setp.lt.u32 	%p373, %r237, 12;
	@%p373 bra 	$L__BB0_196;

$L__BB0_195:
	sub.s64 	%rd819, %rd1154, %rd142;
	mov.b64 	%fd404, %rd819;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r917}, %fd404;
	}
	setp.lt.s32 	%p374, %r917, 0;
	selp.b64 	%rd820, %rd1154, %rd819, %p374;
	shl.b64 	%rd821, %rd820, 1;
	sub.s64 	%rd822, %rd821, %rd142;
	mov.b64 	%fd405, %rd822;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r918}, %fd405;
	}
	setp.lt.s32 	%p375, %r918, 0;
	selp.b64 	%rd823, %rd821, %rd822, %p375;
	shl.b64 	%rd824, %rd823, 1;
	sub.s64 	%rd825, %rd824, %rd142;
	mov.b64 	%fd406, %rd825;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r919}, %fd406;
	}
	setp.lt.s32 	%p376, %r919, 0;
	selp.b64 	%rd826, %rd824, %rd825, %p376;
	shl.b64 	%rd827, %rd826, 1;
	sub.s64 	%rd828, %rd827, %rd142;
	mov.b64 	%fd407, %rd828;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r920}, %fd407;
	}
	setp.lt.s32 	%p377, %r920, 0;
	selp.b64 	%rd829, %rd827, %rd828, %p377;
	shl.b64 	%rd830, %rd829, 1;
	sub.s64 	%rd831, %rd830, %rd142;
	mov.b64 	%fd408, %rd831;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r921}, %fd408;
	}
	setp.lt.s32 	%p378, %r921, 0;
	selp.b64 	%rd832, %rd830, %rd831, %p378;
	shl.b64 	%rd833, %rd832, 1;
	sub.s64 	%rd834, %rd833, %rd142;
	mov.b64 	%fd409, %rd834;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r922}, %fd409;
	}
	setp.lt.s32 	%p379, %r922, 0;
	selp.b64 	%rd835, %rd833, %rd834, %p379;
	shl.b64 	%rd836, %rd835, 1;
	sub.s64 	%rd837, %rd836, %rd142;
	mov.b64 	%fd410, %rd837;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r923}, %fd410;
	}
	setp.lt.s32 	%p380, %r923, 0;
	selp.b64 	%rd838, %rd836, %rd837, %p380;
	shl.b64 	%rd839, %rd838, 1;
	sub.s64 	%rd840, %rd839, %rd142;
	mov.b64 	%fd411, %rd840;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r924}, %fd411;
	}
	setp.lt.s32 	%p381, %r924, 0;
	selp.b64 	%rd841, %rd839, %rd840, %p381;
	shl.b64 	%rd842, %rd841, 1;
	sub.s64 	%rd843, %rd842, %rd142;
	mov.b64 	%fd412, %rd843;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r925}, %fd412;
	}
	setp.lt.s32 	%p382, %r925, 0;
	selp.b64 	%rd844, %rd842, %rd843, %p382;
	shl.b64 	%rd845, %rd844, 1;
	sub.s64 	%rd846, %rd845, %rd142;
	mov.b64 	%fd413, %rd846;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r926}, %fd413;
	}
	setp.lt.s32 	%p383, %r926, 0;
	selp.b64 	%rd847, %rd845, %rd846, %p383;
	shl.b64 	%rd848, %rd847, 1;
	sub.s64 	%rd849, %rd848, %rd142;
	mov.b64 	%fd414, %rd849;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r927}, %fd414;
	}
	setp.lt.s32 	%p384, %r927, 0;
	selp.b64 	%rd850, %rd848, %rd849, %p384;
	shl.b64 	%rd851, %rd850, 1;
	sub.s64 	%rd852, %rd851, %rd142;
	mov.b64 	%fd415, %rd852;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r928}, %fd415;
	}
	setp.lt.s32 	%p385, %r928, 0;
	selp.b64 	%rd853, %rd851, %rd852, %p385;
	shl.b64 	%rd854, %rd853, 1;
	sub.s64 	%rd855, %rd854, %rd142;
	mov.b64 	%fd416, %rd855;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r929}, %fd416;
	}
	setp.lt.s32 	%p386, %r929, 0;
	selp.b64 	%rd856, %rd854, %rd855, %p386;
	shl.b64 	%rd857, %rd856, 1;
	sub.s64 	%rd858, %rd857, %rd142;
	mov.b64 	%fd417, %rd858;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r930}, %fd417;
	}
	setp.lt.s32 	%p387, %r930, 0;
	selp.b64 	%rd859, %rd857, %rd858, %p387;
	shl.b64 	%rd860, %rd859, 1;
	sub.s64 	%rd861, %rd860, %rd142;
	mov.b64 	%fd418, %rd861;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r931}, %fd418;
	}
	setp.lt.s32 	%p388, %r931, 0;
	selp.b64 	%rd862, %rd860, %rd861, %p388;
	shl.b64 	%rd863, %rd862, 1;
	sub.s64 	%rd864, %rd863, %rd142;
	mov.b64 	%fd419, %rd864;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r932}, %fd419;
	}
	setp.lt.s32 	%p389, %r932, 0;
	selp.b64 	%rd1157, %rd863, %rd864, %p389;
	shl.b64 	%rd1154, %rd1157, 1;
	add.s32 	%r245, %r1222, -16;
	setp.gt.s32 	%p390, %r1222, 15;
	mov.u32 	%r1222, %r245;
	@%p390 bra 	$L__BB0_195;

$L__BB0_196:
	and.b64  	%rd157, %rd1157, 9223372036854775807;
	setp.eq.s64 	%p391, %rd157, 0;
	mov.f64 	%fd552, 0d0000000000000000;
	@%p391 bra 	$L__BB0_198;

	mov.b64 	%fd421, %rd157;
	mul.f64 	%fd422, %fd421, 0d4350000000000000;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r933}, %fd422;
	}
	shr.u32 	%r934, %r933, 20;
	mov.u32 	%r935, 55;
	sub.s32 	%r936, %r935, %r934;
	sub.s32 	%r937, %r1216, %r936;
	shl.b64 	%rd865, %rd157, %r936;
	setp.lt.s32 	%p392, %r937, 1;
	mov.u32 	%r938, 1;
	sub.s32 	%r939, %r938, %r937;
	shr.u64 	%rd866, %rd865, %r939;
	add.s32 	%r940, %r937, -1;
	cvt.u64.u32 	%rd867, %r940;
	shl.b64 	%rd868, %rd867, 52;
	add.s64 	%rd869, %rd868, %rd865;
	selp.b64 	%rd870, %rd866, %rd869, %p392;
	mov.b64 	%fd552, %rd870;

$L__BB0_198:
	and.b32  	%r941, %r222, -2147483648;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r942}, %fd552;
	}
	or.b32  	%r943, %r942, %r941;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r944, %temp}, %fd552;
	}
	mov.b64 	%fd553, {%r944, %r943};
	bra.uni 	$L__BB0_202;

$L__BB0_200:
	mov.f64 	%fd423, 0d3FF0000000000000;
	add.rn.f64 	%fd553, %fd102, %fd423;

$L__BB0_202:
	add.s32 	%r1213, %r1214, -1;
	setp.gt.s32 	%p397, %r1214, 0;
	@%p397 bra 	$L__BB0_182;

	fma.rn.f64 	%fd113, %fd553, 0d3FFB96C889463A4C, 0d4001135C51C7F716;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r247}, %fd113;
	}
	and.b32  	%r945, %r247, 2147483647;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r946, %temp}, %fd113;
	}
	mov.b64 	%fd554, {%r946, %r945};
	setp.gt.u32 	%p399, %r945, 2146435071;
	or.pred  	%p400, %p399, %p357;
	@%p400 bra 	$L__BB0_220;
	bra.uni 	$L__BB0_204;

$L__BB0_220:
	setp.le.f64 	%p435, %fd564, 0d7FF0000000000000;
	setp.le.f64 	%p436, %fd554, 0d7FF0000000000000;
	and.pred  	%p437, %p436, %p435;
	@%p437 bra 	$L__BB0_222;
	bra.uni 	$L__BB0_221;

$L__BB0_222:
	setp.eq.f64 	%p438, %fd554, 0d7FF0000000000000;
	selp.f64 	%fd557, 0dFFF8000000000000, %fd113, %p438;
	bra.uni 	$L__BB0_223;

$L__BB0_204:
	setp.eq.f64 	%p401, %fd564, 0d0000000000000000;
	mov.f64 	%fd557, 0dFFF8000000000000;
	@%p401 bra 	$L__BB0_223;

	setp.ltu.f64 	%p402, %fd554, %fd564;
	mov.f64 	%fd557, %fd113;
	@%p402 bra 	$L__BB0_223;

	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r947}, %fd554;
	}
	shr.u32 	%r1224, %r947, 20;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r948}, %fd564;
	}
	shr.u32 	%r1225, %r948, 20;
	setp.ne.s32 	%p403, %r1224, 0;
	@%p403 bra 	$L__BB0_208;

	mul.f64 	%fd554, %fd554, 0d4350000000000000;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r949}, %fd554;
	}
	shr.u32 	%r950, %r949, 20;
	add.s32 	%r1224, %r950, -54;

$L__BB0_208:
	setp.ne.s32 	%p404, %r1225, 0;
	mov.f64 	%fd555, %fd564;
	@%p404 bra 	$L__BB0_210;

	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r951}, %fd100;
	}
	shr.u32 	%r952, %r951, 20;
	add.s32 	%r1225, %r952, -54;
	mov.f64 	%fd555, %fd100;

$L__BB0_210:
	mov.b64 	%rd872, %fd554;
	and.b64  	%rd873, %rd872, 4503599627370495;
	or.b64  	%rd1162, %rd873, 4503599627370496;
	mov.b64 	%rd874, %fd555;
	and.b64  	%rd875, %rd874, 4503599627370495;
	or.b64  	%rd159, %rd875, 4503599627370496;
	sub.s32 	%r1231, %r1224, %r1225;
	not.b32 	%r953, %r1224;
	add.s32 	%r954, %r1225, %r953;
	max.s32 	%r955, %r954, -1;
	add.s32 	%r255, %r955, %r1224;
	mov.u32 	%r956, 2;
	sub.s32 	%r957, %r956, %r1225;
	add.s32 	%r958, %r957, %r255;
	and.b32  	%r1227, %r958, 3;
	setp.eq.s32 	%p405, %r1227, 0;
	@%p405 bra 	$L__BB0_212;

$L__BB0_211:
	.pragma "nounroll";
	sub.s64 	%rd876, %rd1162, %rd159;
	mov.b64 	%fd425, %rd876;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r959}, %fd425;
	}
	setp.lt.s32 	%p406, %r959, 0;
	selp.b64 	%rd1165, %rd1162, %rd876, %p406;
	shl.b64 	%rd1162, %rd1165, 1;
	add.s32 	%r1231, %r1231, -1;
	add.s32 	%r1227, %r1227, -1;
	setp.ne.s32 	%p407, %r1227, 0;
	@%p407 bra 	$L__BB0_211;

$L__BB0_212:
	mov.u32 	%r960, 1;
	sub.s32 	%r961, %r960, %r1225;
	add.s32 	%r962, %r961, %r255;
	setp.lt.u32 	%p408, %r962, 3;
	@%p408 bra 	$L__BB0_217;

	not.b32 	%r963, %r1231;
	max.s32 	%r964, %r963, -4;
	add.s32 	%r965, %r1231, %r964;
	add.s32 	%r262, %r965, 4;
	shr.u32 	%r966, %r262, 2;
	add.s32 	%r967, %r966, 1;
	and.b32  	%r1230, %r967, 3;
	setp.eq.s32 	%p409, %r1230, 0;
	@%p409 bra 	$L__BB0_215;

$L__BB0_214:
	.pragma "nounroll";
	sub.s64 	%rd878, %rd1162, %rd159;
	mov.b64 	%fd426, %rd878;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r968}, %fd426;
	}
	setp.lt.s32 	%p410, %r968, 0;
	selp.b64 	%rd879, %rd1162, %rd878, %p410;
	shl.b64 	%rd880, %rd879, 1;
	sub.s64 	%rd881, %rd880, %rd159;
	mov.b64 	%fd427, %rd881;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r969}, %fd427;
	}
	setp.lt.s32 	%p411, %r969, 0;
	selp.b64 	%rd882, %rd880, %rd881, %p411;
	shl.b64 	%rd883, %rd882, 1;
	sub.s64 	%rd884, %rd883, %rd159;
	mov.b64 	%fd428, %rd884;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r970}, %fd428;
	}
	setp.lt.s32 	%p412, %r970, 0;
	selp.b64 	%rd885, %rd883, %rd884, %p412;
	shl.b64 	%rd886, %rd885, 1;
	sub.s64 	%rd887, %rd886, %rd159;
	mov.b64 	%fd429, %rd887;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r971}, %fd429;
	}
	setp.lt.s32 	%p413, %r971, 0;
	selp.b64 	%rd1165, %rd886, %rd887, %p413;
	shl.b64 	%rd1162, %rd1165, 1;
	add.s32 	%r1231, %r1231, -4;
	add.s32 	%r1230, %r1230, -1;
	setp.ne.s32 	%p414, %r1230, 0;
	@%p414 bra 	$L__BB0_214;

$L__BB0_215:
	setp.lt.u32 	%p415, %r262, 12;
	@%p415 bra 	$L__BB0_217;

$L__BB0_216:
	sub.s64 	%rd888, %rd1162, %rd159;
	mov.b64 	%fd430, %rd888;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r972}, %fd430;
	}
	setp.lt.s32 	%p416, %r972, 0;
	selp.b64 	%rd889, %rd1162, %rd888, %p416;
	shl.b64 	%rd890, %rd889, 1;
	sub.s64 	%rd891, %rd890, %rd159;
	mov.b64 	%fd431, %rd891;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r973}, %fd431;
	}
	setp.lt.s32 	%p417, %r973, 0;
	selp.b64 	%rd892, %rd890, %rd891, %p417;
	shl.b64 	%rd893, %rd892, 1;
	sub.s64 	%rd894, %rd893, %rd159;
	mov.b64 	%fd432, %rd894;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r974}, %fd432;
	}
	setp.lt.s32 	%p418, %r974, 0;
	selp.b64 	%rd895, %rd893, %rd894, %p418;
	shl.b64 	%rd896, %rd895, 1;
	sub.s64 	%rd897, %rd896, %rd159;
	mov.b64 	%fd433, %rd897;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r975}, %fd433;
	}
	setp.lt.s32 	%p419, %r975, 0;
	selp.b64 	%rd898, %rd896, %rd897, %p419;
	shl.b64 	%rd899, %rd898, 1;
	sub.s64 	%rd900, %rd899, %rd159;
	mov.b64 	%fd434, %rd900;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r976}, %fd434;
	}
	setp.lt.s32 	%p420, %r976, 0;
	selp.b64 	%rd901, %rd899, %rd900, %p420;
	shl.b64 	%rd902, %rd901, 1;
	sub.s64 	%rd903, %rd902, %rd159;
	mov.b64 	%fd435, %rd903;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r977}, %fd435;
	}
	setp.lt.s32 	%p421, %r977, 0;
	selp.b64 	%rd904, %rd902, %rd903, %p421;
	shl.b64 	%rd905, %rd904, 1;
	sub.s64 	%rd906, %rd905, %rd159;
	mov.b64 	%fd436, %rd906;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r978}, %fd436;
	}
	setp.lt.s32 	%p422, %r978, 0;
	selp.b64 	%rd907, %rd905, %rd906, %p422;
	shl.b64 	%rd908, %rd907, 1;
	sub.s64 	%rd909, %rd908, %rd159;
	mov.b64 	%fd437, %rd909;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r979}, %fd437;
	}
	setp.lt.s32 	%p423, %r979, 0;
	selp.b64 	%rd910, %rd908, %rd909, %p423;
	shl.b64 	%rd911, %rd910, 1;
	sub.s64 	%rd912, %rd911, %rd159;
	mov.b64 	%fd438, %rd912;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r980}, %fd438;
	}
	setp.lt.s32 	%p424, %r980, 0;
	selp.b64 	%rd913, %rd911, %rd912, %p424;
	shl.b64 	%rd914, %rd913, 1;
	sub.s64 	%rd915, %rd914, %rd159;
	mov.b64 	%fd439, %rd915;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r981}, %fd439;
	}
	setp.lt.s32 	%p425, %r981, 0;
	selp.b64 	%rd916, %rd914, %rd915, %p425;
	shl.b64 	%rd917, %rd916, 1;
	sub.s64 	%rd918, %rd917, %rd159;
	mov.b64 	%fd440, %rd918;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r982}, %fd440;
	}
	setp.lt.s32 	%p426, %r982, 0;
	selp.b64 	%rd919, %rd917, %rd918, %p426;
	shl.b64 	%rd920, %rd919, 1;
	sub.s64 	%rd921, %rd920, %rd159;
	mov.b64 	%fd441, %rd921;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r983}, %fd441;
	}
	setp.lt.s32 	%p427, %r983, 0;
	selp.b64 	%rd922, %rd920, %rd921, %p427;
	shl.b64 	%rd923, %rd922, 1;
	sub.s64 	%rd924, %rd923, %rd159;
	mov.b64 	%fd442, %rd924;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r984}, %fd442;
	}
	setp.lt.s32 	%p428, %r984, 0;
	selp.b64 	%rd925, %rd923, %rd924, %p428;
	shl.b64 	%rd926, %rd925, 1;
	sub.s64 	%rd927, %rd926, %rd159;
	mov.b64 	%fd443, %rd927;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r985}, %fd443;
	}
	setp.lt.s32 	%p429, %r985, 0;
	selp.b64 	%rd928, %rd926, %rd927, %p429;
	shl.b64 	%rd929, %rd928, 1;
	sub.s64 	%rd930, %rd929, %rd159;
	mov.b64 	%fd444, %rd930;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r986}, %fd444;
	}
	setp.lt.s32 	%p430, %r986, 0;
	selp.b64 	%rd931, %rd929, %rd930, %p430;
	shl.b64 	%rd932, %rd931, 1;
	sub.s64 	%rd933, %rd932, %rd159;
	mov.b64 	%fd445, %rd933;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r987}, %fd445;
	}
	setp.lt.s32 	%p431, %r987, 0;
	selp.b64 	%rd1165, %rd932, %rd933, %p431;
	shl.b64 	%rd1162, %rd1165, 1;
	add.s32 	%r270, %r1231, -16;
	setp.gt.s32 	%p432, %r1231, 15;
	mov.u32 	%r1231, %r270;
	@%p432 bra 	$L__BB0_216;

$L__BB0_217:
	and.b64  	%rd174, %rd1165, 9223372036854775807;
	setp.eq.s64 	%p433, %rd174, 0;
	mov.f64 	%fd556, 0d0000000000000000;
	@%p433 bra 	$L__BB0_219;

	mov.b64 	%fd447, %rd174;
	mul.f64 	%fd448, %fd447, 0d4350000000000000;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r988}, %fd448;
	}
	shr.u32 	%r989, %r988, 20;
	mov.u32 	%r990, 55;
	sub.s32 	%r991, %r990, %r989;
	sub.s32 	%r992, %r1225, %r991;
	shl.b64 	%rd934, %rd174, %r991;
	setp.lt.s32 	%p434, %r992, 1;
	mov.u32 	%r993, 1;
	sub.s32 	%r994, %r993, %r992;
	shr.u64 	%rd935, %rd934, %r994;
	add.s32 	%r995, %r992, -1;
	cvt.u64.u32 	%rd936, %r995;
	shl.b64 	%rd937, %rd936, 52;
	add.s64 	%rd938, %rd937, %rd934;
	selp.b64 	%rd939, %rd935, %rd938, %p434;
	mov.b64 	%fd556, %rd939;

$L__BB0_219:
	and.b32  	%r996, %r247, -2147483648;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r997}, %fd556;
	}
	or.b32  	%r998, %r997, %r996;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r999, %temp}, %fd556;
	}
	mov.b64 	%fd557, {%r999, %r998};
	bra.uni 	$L__BB0_223;

$L__BB0_221:
	mov.f64 	%fd449, 0d3FF0000000000000;
	add.rn.f64 	%fd557, %fd113, %fd449;

$L__BB0_223:
	abs.f64 	%fd451, %fd557;
	mul.f64 	%fd452, %fd451, 0d41EFFFFFFFE00000;
	cvt.rzi.u32.f64 	%r1002, %fd452;
	and.b32  	%r271, %r1002, 31;
	mov.u32 	%r1003, 1060385854;
	mov.u32 	%r1004, 808662589;
	st.local.v4.u32 	[%rd2], {%r1004, %r1003, %r217, %r218};
	mov.f64 	%fd562, 0d3FF0000000000000;
	mov.u32 	%r1234, 16;
	mov.u32 	%r1233, 15;

$L__BB0_224:
	mov.u32 	%r273, %r1234;
	mov.u32 	%r1234, %r1233;
	cvt.s64.s32 	%rd940, %r1234;
	add.s64 	%rd941, %rd2, %rd940;
	ld.local.s8 	%rs31, [%rd941];
	cvt.rn.f64.s16 	%fd453, %rs31;
	mov.f64 	%fd454, 0d3FF1FB9C7406BB70;
	div.rn.f64 	%fd455, %fd454, %fd562;
	mul.f64 	%fd456, %fd455, %fd453;
	mul.f64 	%fd457, %fd456, 0d400921FB54442EEA;
	cvt.rn.f64.s32 	%fd458, %r273;
	fma.rn.f64 	%fd125, %fd458, 0d400921FB54442EEA, %fd457;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r274}, %fd125;
	}
	and.b32  	%r1005, %r274, 2147483647;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r1006, %temp}, %fd125;
	}
	mov.b64 	%fd559, {%r1006, %r1005};
	setp.gt.u32 	%p439, %r1005, 2146435071;
	or.pred  	%p441, %p439, %p357;
	@%p441 bra 	$L__BB0_241;
	bra.uni 	$L__BB0_225;

$L__BB0_241:
	setp.le.f64 	%p476, %fd564, 0d7FF0000000000000;
	setp.le.f64 	%p477, %fd559, 0d7FF0000000000000;
	and.pred  	%p478, %p477, %p476;
	@%p478 bra 	$L__BB0_243;
	bra.uni 	$L__BB0_242;

$L__BB0_243:
	setp.eq.f64 	%p479, %fd559, 0d7FF0000000000000;
	selp.f64 	%fd562, 0dFFF8000000000000, %fd125, %p479;
	bra.uni 	$L__BB0_244;

$L__BB0_225:
	setp.eq.f64 	%p442, %fd564, 0d0000000000000000;
	mov.f64 	%fd562, 0dFFF8000000000000;
	@%p442 bra 	$L__BB0_244;

	setp.ltu.f64 	%p443, %fd559, %fd564;
	mov.f64 	%fd562, %fd125;
	@%p443 bra 	$L__BB0_244;

	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r1007}, %fd559;
	}
	shr.u32 	%r1235, %r1007, 20;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r1008}, %fd564;
	}
	shr.u32 	%r1236, %r1008, 20;
	setp.ne.s32 	%p444, %r1235, 0;
	@%p444 bra 	$L__BB0_229;

	mul.f64 	%fd559, %fd559, 0d4350000000000000;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r1009}, %fd559;
	}
	shr.u32 	%r1010, %r1009, 20;
	add.s32 	%r1235, %r1010, -54;

$L__BB0_229:
	setp.ne.s32 	%p445, %r1236, 0;
	mov.f64 	%fd560, %fd564;
	@%p445 bra 	$L__BB0_231;

	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r1011}, %fd100;
	}
	shr.u32 	%r1012, %r1011, 20;
	add.s32 	%r1236, %r1012, -54;
	mov.f64 	%fd560, %fd100;

$L__BB0_231:
	mov.b64 	%rd943, %fd559;
	and.b64  	%rd944, %rd943, 4503599627370495;
	or.b64  	%rd1170, %rd944, 4503599627370496;
	mov.b64 	%rd945, %fd560;
	and.b64  	%rd946, %rd945, 4503599627370495;
	or.b64  	%rd176, %rd946, 4503599627370496;
	sub.s32 	%r1242, %r1235, %r1236;
	not.b32 	%r1013, %r1235;
	add.s32 	%r1014, %r1236, %r1013;
	max.s32 	%r1015, %r1014, -1;
	add.s32 	%r282, %r1015, %r1235;
	mov.u32 	%r1016, 2;
	sub.s32 	%r1017, %r1016, %r1236;
	add.s32 	%r1018, %r1017, %r282;
	and.b32  	%r1238, %r1018, 3;
	setp.eq.s32 	%p446, %r1238, 0;
	@%p446 bra 	$L__BB0_233;

$L__BB0_232:
	.pragma "nounroll";
	sub.s64 	%rd947, %rd1170, %rd176;
	mov.b64 	%fd460, %rd947;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r1019}, %fd460;
	}
	setp.lt.s32 	%p447, %r1019, 0;
	selp.b64 	%rd1173, %rd1170, %rd947, %p447;
	shl.b64 	%rd1170, %rd1173, 1;
	add.s32 	%r1242, %r1242, -1;
	add.s32 	%r1238, %r1238, -1;
	setp.ne.s32 	%p448, %r1238, 0;
	@%p448 bra 	$L__BB0_232;

$L__BB0_233:
	mov.u32 	%r1020, 1;
	sub.s32 	%r1021, %r1020, %r1236;
	add.s32 	%r1022, %r1021, %r282;
	setp.lt.u32 	%p449, %r1022, 3;
	@%p449 bra 	$L__BB0_238;

	not.b32 	%r1023, %r1242;
	max.s32 	%r1024, %r1023, -4;
	add.s32 	%r1025, %r1242, %r1024;
	add.s32 	%r289, %r1025, 4;
	shr.u32 	%r1026, %r289, 2;
	add.s32 	%r1027, %r1026, 1;
	and.b32  	%r1241, %r1027, 3;
	setp.eq.s32 	%p450, %r1241, 0;
	@%p450 bra 	$L__BB0_236;

$L__BB0_235:
	.pragma "nounroll";
	sub.s64 	%rd949, %rd1170, %rd176;
	mov.b64 	%fd461, %rd949;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r1028}, %fd461;
	}
	setp.lt.s32 	%p451, %r1028, 0;
	selp.b64 	%rd950, %rd1170, %rd949, %p451;
	shl.b64 	%rd951, %rd950, 1;
	sub.s64 	%rd952, %rd951, %rd176;
	mov.b64 	%fd462, %rd952;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r1029}, %fd462;
	}
	setp.lt.s32 	%p452, %r1029, 0;
	selp.b64 	%rd953, %rd951, %rd952, %p452;
	shl.b64 	%rd954, %rd953, 1;
	sub.s64 	%rd955, %rd954, %rd176;
	mov.b64 	%fd463, %rd955;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r1030}, %fd463;
	}
	setp.lt.s32 	%p453, %r1030, 0;
	selp.b64 	%rd956, %rd954, %rd955, %p453;
	shl.b64 	%rd957, %rd956, 1;
	sub.s64 	%rd958, %rd957, %rd176;
	mov.b64 	%fd464, %rd958;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r1031}, %fd464;
	}
	setp.lt.s32 	%p454, %r1031, 0;
	selp.b64 	%rd1173, %rd957, %rd958, %p454;
	shl.b64 	%rd1170, %rd1173, 1;
	add.s32 	%r1242, %r1242, -4;
	add.s32 	%r1241, %r1241, -1;
	setp.ne.s32 	%p455, %r1241, 0;
	@%p455 bra 	$L__BB0_235;

$L__BB0_236:
	setp.lt.u32 	%p456, %r289, 12;
	@%p456 bra 	$L__BB0_238;

$L__BB0_237:
	sub.s64 	%rd959, %rd1170, %rd176;
	mov.b64 	%fd465, %rd959;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r1032}, %fd465;
	}
	setp.lt.s32 	%p457, %r1032, 0;
	selp.b64 	%rd960, %rd1170, %rd959, %p457;
	shl.b64 	%rd961, %rd960, 1;
	sub.s64 	%rd962, %rd961, %rd176;
	mov.b64 	%fd466, %rd962;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r1033}, %fd466;
	}
	setp.lt.s32 	%p458, %r1033, 0;
	selp.b64 	%rd963, %rd961, %rd962, %p458;
	shl.b64 	%rd964, %rd963, 1;
	sub.s64 	%rd965, %rd964, %rd176;
	mov.b64 	%fd467, %rd965;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r1034}, %fd467;
	}
	setp.lt.s32 	%p459, %r1034, 0;
	selp.b64 	%rd966, %rd964, %rd965, %p459;
	shl.b64 	%rd967, %rd966, 1;
	sub.s64 	%rd968, %rd967, %rd176;
	mov.b64 	%fd468, %rd968;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r1035}, %fd468;
	}
	setp.lt.s32 	%p460, %r1035, 0;
	selp.b64 	%rd969, %rd967, %rd968, %p460;
	shl.b64 	%rd970, %rd969, 1;
	sub.s64 	%rd971, %rd970, %rd176;
	mov.b64 	%fd469, %rd971;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r1036}, %fd469;
	}
	setp.lt.s32 	%p461, %r1036, 0;
	selp.b64 	%rd972, %rd970, %rd971, %p461;
	shl.b64 	%rd973, %rd972, 1;
	sub.s64 	%rd974, %rd973, %rd176;
	mov.b64 	%fd470, %rd974;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r1037}, %fd470;
	}
	setp.lt.s32 	%p462, %r1037, 0;
	selp.b64 	%rd975, %rd973, %rd974, %p462;
	shl.b64 	%rd976, %rd975, 1;
	sub.s64 	%rd977, %rd976, %rd176;
	mov.b64 	%fd471, %rd977;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r1038}, %fd471;
	}
	setp.lt.s32 	%p463, %r1038, 0;
	selp.b64 	%rd978, %rd976, %rd977, %p463;
	shl.b64 	%rd979, %rd978, 1;
	sub.s64 	%rd980, %rd979, %rd176;
	mov.b64 	%fd472, %rd980;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r1039}, %fd472;
	}
	setp.lt.s32 	%p464, %r1039, 0;
	selp.b64 	%rd981, %rd979, %rd980, %p464;
	shl.b64 	%rd982, %rd981, 1;
	sub.s64 	%rd983, %rd982, %rd176;
	mov.b64 	%fd473, %rd983;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r1040}, %fd473;
	}
	setp.lt.s32 	%p465, %r1040, 0;
	selp.b64 	%rd984, %rd982, %rd983, %p465;
	shl.b64 	%rd985, %rd984, 1;
	sub.s64 	%rd986, %rd985, %rd176;
	mov.b64 	%fd474, %rd986;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r1041}, %fd474;
	}
	setp.lt.s32 	%p466, %r1041, 0;
	selp.b64 	%rd987, %rd985, %rd986, %p466;
	shl.b64 	%rd988, %rd987, 1;
	sub.s64 	%rd989, %rd988, %rd176;
	mov.b64 	%fd475, %rd989;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r1042}, %fd475;
	}
	setp.lt.s32 	%p467, %r1042, 0;
	selp.b64 	%rd990, %rd988, %rd989, %p467;
	shl.b64 	%rd991, %rd990, 1;
	sub.s64 	%rd992, %rd991, %rd176;
	mov.b64 	%fd476, %rd992;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r1043}, %fd476;
	}
	setp.lt.s32 	%p468, %r1043, 0;
	selp.b64 	%rd993, %rd991, %rd992, %p468;
	shl.b64 	%rd994, %rd993, 1;
	sub.s64 	%rd995, %rd994, %rd176;
	mov.b64 	%fd477, %rd995;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r1044}, %fd477;
	}
	setp.lt.s32 	%p469, %r1044, 0;
	selp.b64 	%rd996, %rd994, %rd995, %p469;
	shl.b64 	%rd997, %rd996, 1;
	sub.s64 	%rd998, %rd997, %rd176;
	mov.b64 	%fd478, %rd998;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r1045}, %fd478;
	}
	setp.lt.s32 	%p470, %r1045, 0;
	selp.b64 	%rd999, %rd997, %rd998, %p470;
	shl.b64 	%rd1000, %rd999, 1;
	sub.s64 	%rd1001, %rd1000, %rd176;
	mov.b64 	%fd479, %rd1001;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r1046}, %fd479;
	}
	setp.lt.s32 	%p471, %r1046, 0;
	selp.b64 	%rd1002, %rd1000, %rd1001, %p471;
	shl.b64 	%rd1003, %rd1002, 1;
	sub.s64 	%rd1004, %rd1003, %rd176;
	mov.b64 	%fd480, %rd1004;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r1047}, %fd480;
	}
	setp.lt.s32 	%p472, %r1047, 0;
	selp.b64 	%rd1173, %rd1003, %rd1004, %p472;
	shl.b64 	%rd1170, %rd1173, 1;
	add.s32 	%r297, %r1242, -16;
	setp.gt.s32 	%p473, %r1242, 15;
	mov.u32 	%r1242, %r297;
	@%p473 bra 	$L__BB0_237;

$L__BB0_238:
	and.b64  	%rd191, %rd1173, 9223372036854775807;
	setp.eq.s64 	%p474, %rd191, 0;
	mov.f64 	%fd561, 0d0000000000000000;
	@%p474 bra 	$L__BB0_240;

	mov.b64 	%fd482, %rd191;
	mul.f64 	%fd483, %fd482, 0d4350000000000000;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r1048}, %fd483;
	}
	shr.u32 	%r1049, %r1048, 20;
	mov.u32 	%r1050, 55;
	sub.s32 	%r1051, %r1050, %r1049;
	sub.s32 	%r1052, %r1236, %r1051;
	shl.b64 	%rd1005, %rd191, %r1051;
	setp.lt.s32 	%p475, %r1052, 1;
	mov.u32 	%r1053, 1;
	sub.s32 	%r1054, %r1053, %r1052;
	shr.u64 	%rd1006, %rd1005, %r1054;
	add.s32 	%r1055, %r1052, -1;
	cvt.u64.u32 	%rd1007, %r1055;
	shl.b64 	%rd1008, %rd1007, 52;
	add.s64 	%rd1009, %rd1008, %rd1005;
	selp.b64 	%rd1010, %rd1006, %rd1009, %p475;
	mov.b64 	%fd561, %rd1010;

$L__BB0_240:
	and.b32  	%r1056, %r274, -2147483648;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r1057}, %fd561;
	}
	or.b32  	%r1058, %r1057, %r1056;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r1059, %temp}, %fd561;
	}
	mov.b64 	%fd562, {%r1059, %r1058};
	bra.uni 	$L__BB0_244;

$L__BB0_242:
	mov.f64 	%fd484, 0d3FF0000000000000;
	add.rn.f64 	%fd562, %fd125, %fd484;

$L__BB0_244:
	add.s32 	%r1233, %r1234, -1;
	setp.gt.s32 	%p480, %r1234, 0;
	@%p480 bra 	$L__BB0_224;

	fma.rn.f64 	%fd136, %fd562, 0d3FFB96C889463A4C, 0d4001135C51C7F716;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r299}, %fd136;
	}
	and.b32  	%r1060, %r299, 2147483647;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r1061, %temp}, %fd136;
	}
	mov.b64 	%fd563, {%r1061, %r1060};
	setp.gt.u32 	%p482, %r1060, 2146435071;
	or.pred  	%p483, %p482, %p357;
	@%p483 bra 	$L__BB0_263;
	bra.uni 	$L__BB0_246;

$L__BB0_263:
	setp.le.f64 	%p518, %fd564, 0d7FF0000000000000;
	setp.le.f64 	%p519, %fd563, 0d7FF0000000000000;
	and.pred  	%p520, %p519, %p518;
	@%p520 bra 	$L__BB0_265;
	bra.uni 	$L__BB0_264;

$L__BB0_265:
	setp.eq.f64 	%p521, %fd563, 0d7FF0000000000000;
	selp.f64 	%fd566, 0dFFF8000000000000, %fd136, %p521;
	bra.uni 	$L__BB0_266;

$L__BB0_246:
	setp.eq.f64 	%p484, %fd564, 0d0000000000000000;
	mov.f64 	%fd566, 0dFFF8000000000000;
	@%p484 bra 	$L__BB0_266;

	setp.ltu.f64 	%p485, %fd563, %fd564;
	mov.f64 	%fd566, %fd136;
	@%p485 bra 	$L__BB0_266;

	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r1062}, %fd563;
	}
	shr.u32 	%r1244, %r1062, 20;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r1063}, %fd564;
	}
	shr.u32 	%r1245, %r1063, 20;
	setp.ne.s32 	%p486, %r1244, 0;
	@%p486 bra 	$L__BB0_250;

	mul.f64 	%fd563, %fd563, 0d4350000000000000;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r1064}, %fd563;
	}
	shr.u32 	%r1065, %r1064, 20;
	add.s32 	%r1244, %r1065, -54;

$L__BB0_250:
	setp.ne.s32 	%p487, %r1245, 0;
	@%p487 bra 	$L__BB0_252;

	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r1066}, %fd100;
	}
	shr.u32 	%r1067, %r1066, 20;
	add.s32 	%r1245, %r1067, -54;
	mov.f64 	%fd564, %fd100;

$L__BB0_252:
	mov.b64 	%rd1012, %fd563;
	and.b64  	%rd1013, %rd1012, 4503599627370495;
	or.b64  	%rd1178, %rd1013, 4503599627370496;
	mov.b64 	%rd1014, %fd564;
	and.b64  	%rd1015, %rd1014, 4503599627370495;
	or.b64  	%rd193, %rd1015, 4503599627370496;
	sub.s32 	%r1251, %r1244, %r1245;
	not.b32 	%r1068, %r1244;
	add.s32 	%r1069, %r1245, %r1068;
	max.s32 	%r1070, %r1069, -1;
	add.s32 	%r307, %r1070, %r1244;
	mov.u32 	%r1071, 2;
	sub.s32 	%r1072, %r1071, %r1245;
	add.s32 	%r1073, %r1072, %r307;
	and.b32  	%r1247, %r1073, 3;
	setp.eq.s32 	%p488, %r1247, 0;
	@%p488 bra 	$L__BB0_254;

$L__BB0_253:
	.pragma "nounroll";
	sub.s64 	%rd1016, %rd1178, %rd193;
	mov.b64 	%fd486, %rd1016;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r1074}, %fd486;
	}
	setp.lt.s32 	%p489, %r1074, 0;
	selp.b64 	%rd1181, %rd1178, %rd1016, %p489;
	shl.b64 	%rd1178, %rd1181, 1;
	add.s32 	%r1251, %r1251, -1;
	add.s32 	%r1247, %r1247, -1;
	setp.ne.s32 	%p490, %r1247, 0;
	@%p490 bra 	$L__BB0_253;

$L__BB0_254:
	mov.u32 	%r1075, 1;
	sub.s32 	%r1076, %r1075, %r1245;
	add.s32 	%r1077, %r1076, %r307;
	setp.lt.u32 	%p491, %r1077, 3;
	@%p491 bra 	$L__BB0_260;

	not.b32 	%r1078, %r1251;
	max.s32 	%r1079, %r1078, -4;
	add.s32 	%r1080, %r1251, %r1079;
	add.s32 	%r314, %r1080, 4;
	shr.u32 	%r1081, %r314, 2;
	add.s32 	%r1082, %r1081, 1;
	and.b32  	%r1250, %r1082, 3;
	setp.eq.s32 	%p492, %r1250, 0;
	@%p492 bra 	$L__BB0_257;

$L__BB0_256:
	.pragma "nounroll";
	sub.s64 	%rd1018, %rd1178, %rd193;
	mov.b64 	%fd487, %rd1018;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r1083}, %fd487;
	}
	setp.lt.s32 	%p493, %r1083, 0;
	selp.b64 	%rd1019, %rd1178, %rd1018, %p493;
	shl.b64 	%rd1020, %rd1019, 1;
	sub.s64 	%rd1021, %rd1020, %rd193;
	mov.b64 	%fd488, %rd1021;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r1084}, %fd488;
	}
	setp.lt.s32 	%p494, %r1084, 0;
	selp.b64 	%rd1022, %rd1020, %rd1021, %p494;
	shl.b64 	%rd1023, %rd1022, 1;
	sub.s64 	%rd1024, %rd1023, %rd193;
	mov.b64 	%fd489, %rd1024;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r1085}, %fd489;
	}
	setp.lt.s32 	%p495, %r1085, 0;
	selp.b64 	%rd1025, %rd1023, %rd1024, %p495;
	shl.b64 	%rd1026, %rd1025, 1;
	sub.s64 	%rd1027, %rd1026, %rd193;
	mov.b64 	%fd490, %rd1027;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r1086}, %fd490;
	}
	setp.lt.s32 	%p496, %r1086, 0;
	selp.b64 	%rd1181, %rd1026, %rd1027, %p496;
	shl.b64 	%rd1178, %rd1181, 1;
	add.s32 	%r1251, %r1251, -4;
	add.s32 	%r1250, %r1250, -1;
	setp.ne.s32 	%p497, %r1250, 0;
	@%p497 bra 	$L__BB0_256;

$L__BB0_257:
	setp.lt.u32 	%p498, %r314, 12;
	@%p498 bra 	$L__BB0_260;

	add.s32 	%r1252, %r1251, 16;

$L__BB0_259:
	sub.s64 	%rd1028, %rd1178, %rd193;
	mov.b64 	%fd491, %rd1028;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r1087}, %fd491;
	}
	setp.lt.s32 	%p499, %r1087, 0;
	selp.b64 	%rd1029, %rd1178, %rd1028, %p499;
	shl.b64 	%rd1030, %rd1029, 1;
	sub.s64 	%rd1031, %rd1030, %rd193;
	mov.b64 	%fd492, %rd1031;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r1088}, %fd492;
	}
	setp.lt.s32 	%p500, %r1088, 0;
	selp.b64 	%rd1032, %rd1030, %rd1031, %p500;
	shl.b64 	%rd1033, %rd1032, 1;
	sub.s64 	%rd1034, %rd1033, %rd193;
	mov.b64 	%fd493, %rd1034;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r1089}, %fd493;
	}
	setp.lt.s32 	%p501, %r1089, 0;
	selp.b64 	%rd1035, %rd1033, %rd1034, %p501;
	shl.b64 	%rd1036, %rd1035, 1;
	sub.s64 	%rd1037, %rd1036, %rd193;
	mov.b64 	%fd494, %rd1037;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r1090}, %fd494;
	}
	setp.lt.s32 	%p502, %r1090, 0;
	selp.b64 	%rd1038, %rd1036, %rd1037, %p502;
	shl.b64 	%rd1039, %rd1038, 1;
	sub.s64 	%rd1040, %rd1039, %rd193;
	mov.b64 	%fd495, %rd1040;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r1091}, %fd495;
	}
	setp.lt.s32 	%p503, %r1091, 0;
	selp.b64 	%rd1041, %rd1039, %rd1040, %p503;
	shl.b64 	%rd1042, %rd1041, 1;
	sub.s64 	%rd1043, %rd1042, %rd193;
	mov.b64 	%fd496, %rd1043;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r1092}, %fd496;
	}
	setp.lt.s32 	%p504, %r1092, 0;
	selp.b64 	%rd1044, %rd1042, %rd1043, %p504;
	shl.b64 	%rd1045, %rd1044, 1;
	sub.s64 	%rd1046, %rd1045, %rd193;
	mov.b64 	%fd497, %rd1046;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r1093}, %fd497;
	}
	setp.lt.s32 	%p505, %r1093, 0;
	selp.b64 	%rd1047, %rd1045, %rd1046, %p505;
	shl.b64 	%rd1048, %rd1047, 1;
	sub.s64 	%rd1049, %rd1048, %rd193;
	mov.b64 	%fd498, %rd1049;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r1094}, %fd498;
	}
	setp.lt.s32 	%p506, %r1094, 0;
	selp.b64 	%rd1050, %rd1048, %rd1049, %p506;
	shl.b64 	%rd1051, %rd1050, 1;
	sub.s64 	%rd1052, %rd1051, %rd193;
	mov.b64 	%fd499, %rd1052;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r1095}, %fd499;
	}
	setp.lt.s32 	%p507, %r1095, 0;
	selp.b64 	%rd1053, %rd1051, %rd1052, %p507;
	shl.b64 	%rd1054, %rd1053, 1;
	sub.s64 	%rd1055, %rd1054, %rd193;
	mov.b64 	%fd500, %rd1055;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r1096}, %fd500;
	}
	setp.lt.s32 	%p508, %r1096, 0;
	selp.b64 	%rd1056, %rd1054, %rd1055, %p508;
	shl.b64 	%rd1057, %rd1056, 1;
	sub.s64 	%rd1058, %rd1057, %rd193;
	mov.b64 	%fd501, %rd1058;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r1097}, %fd501;
	}
	setp.lt.s32 	%p509, %r1097, 0;
	selp.b64 	%rd1059, %rd1057, %rd1058, %p509;
	shl.b64 	%rd1060, %rd1059, 1;
	sub.s64 	%rd1061, %rd1060, %rd193;
	mov.b64 	%fd502, %rd1061;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r1098}, %fd502;
	}
	setp.lt.s32 	%p510, %r1098, 0;
	selp.b64 	%rd1062, %rd1060, %rd1061, %p510;
	shl.b64 	%rd1063, %rd1062, 1;
	sub.s64 	%rd1064, %rd1063, %rd193;
	mov.b64 	%fd503, %rd1064;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r1099}, %fd503;
	}
	setp.lt.s32 	%p511, %r1099, 0;
	selp.b64 	%rd1065, %rd1063, %rd1064, %p511;
	shl.b64 	%rd1066, %rd1065, 1;
	sub.s64 	%rd1067, %rd1066, %rd193;
	mov.b64 	%fd504, %rd1067;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r1100}, %fd504;
	}
	setp.lt.s32 	%p512, %r1100, 0;
	selp.b64 	%rd1068, %rd1066, %rd1067, %p512;
	shl.b64 	%rd1069, %rd1068, 1;
	sub.s64 	%rd1070, %rd1069, %rd193;
	mov.b64 	%fd505, %rd1070;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r1101}, %fd505;
	}
	setp.lt.s32 	%p513, %r1101, 0;
	selp.b64 	%rd1071, %rd1069, %rd1070, %p513;
	shl.b64 	%rd1072, %rd1071, 1;
	sub.s64 	%rd1073, %rd1072, %rd193;
	mov.b64 	%fd506, %rd1073;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r1102}, %fd506;
	}
	setp.lt.s32 	%p514, %r1102, 0;
	selp.b64 	%rd1181, %rd1072, %rd1073, %p514;
	shl.b64 	%rd1178, %rd1181, 1;
	add.s32 	%r1252, %r1252, -16;
	setp.gt.s32 	%p515, %r1252, 15;
	@%p515 bra 	$L__BB0_259;

$L__BB0_260:
	and.b64  	%rd208, %rd1181, 9223372036854775807;
	setp.eq.s64 	%p516, %rd208, 0;
	mov.f64 	%fd565, 0d0000000000000000;
	@%p516 bra 	$L__BB0_262;

	mov.b64 	%fd508, %rd208;
	mul.f64 	%fd509, %fd508, 0d4350000000000000;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r1103}, %fd509;
	}
	shr.u32 	%r1104, %r1103, 20;
	mov.u32 	%r1105, 55;
	sub.s32 	%r1106, %r1105, %r1104;
	sub.s32 	%r1107, %r1245, %r1106;
	shl.b64 	%rd1074, %rd208, %r1106;
	setp.lt.s32 	%p517, %r1107, 1;
	mov.u32 	%r1108, 1;
	sub.s32 	%r1109, %r1108, %r1107;
	shr.u64 	%rd1075, %rd1074, %r1109;
	add.s32 	%r1110, %r1107, -1;
	cvt.u64.u32 	%rd1076, %r1110;
	shl.b64 	%rd1077, %rd1076, 52;
	add.s64 	%rd1078, %rd1077, %rd1074;
	selp.b64 	%rd1079, %rd1075, %rd1078, %p517;
	mov.b64 	%fd565, %rd1079;

$L__BB0_262:
	and.b32  	%r1111, %r299, -2147483648;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r1112}, %fd565;
	}
	or.b32  	%r1113, %r1112, %r1111;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r1114, %temp}, %fd565;
	}
	mov.b64 	%fd566, {%r1114, %r1113};
	bra.uni 	$L__BB0_266;

$L__BB0_264:
	mov.f64 	%fd510, 0d3FF0000000000000;
	add.rn.f64 	%fd566, %fd136, %fd510;

$L__BB0_266:
	abs.f64 	%fd511, %fd566;
	mul.f64 	%fd512, %fd511, 0d41EFFFFFFFE00000;
	cvt.rzi.u32.f64 	%r1115, %fd512;
	mul.wide.u32 	%rd1080, %r1115, -2004318071;
	shr.u64 	%rd1081, %rd1080, 35;
	cvt.u32.u64 	%r1116, %rd1081;
	mul.lo.s32 	%r1117, %r1116, 15;
	sub.s32 	%r1118, %r1115, %r1117;
	setp.ne.s32 	%p522, %r1118, 12;
	setp.ne.s32 	%p523, %r271, 24;
	or.pred  	%p524, %p523, %p522;
	@%p524 bra 	$L__BB0_269;

$L__BB0_267:
	ld.param.u64 	%rd1084, [_Z17find_seeds_kernelmj12FilterParamsPmPVi_param_4];
	cvta.to.global.u64 	%rd1083, %rd1084;
	mov.u32 	%r1119, 1;
	mov.u32 	%r1120, 0;
	atom.global.cas.b32 	%r1121, [%rd1083], %r1120, %r1119;
	setp.ne.s32 	%p525, %r1121, 0;
	@%p525 bra 	$L__BB0_269;

	ld.param.u64 	%rd1085, [_Z17find_seeds_kernelmj12FilterParamsPmPVi_param_3];
	cvta.to.global.u64 	%rd1082, %rd1085;
	st.global.u64 	[%rd1082], %rd3;
	bra.uni 	$L__BB0_269;

}

